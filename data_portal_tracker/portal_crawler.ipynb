{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MongoDB...\n",
      "Connection failed.\n"
     ]
    }
   ],
   "source": [
    "# Loading required packages\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from re import search\n",
    "from time import sleep\n",
    "from statistics import mean\n",
    "from datetime import datetime\n",
    "from dotenv import dotenv_values\n",
    "from helpers import check_protocol, remove_double_slashes\n",
    "from archiver_connector import ArchiverConnector\n",
    "\n",
    "# Loading environment variables\n",
    "config = dotenv_values(\"../.env\")\n",
    "\n",
    "# Calling the Archiver connector\n",
    "archiver = ArchiverConnector(mode = \"local\")\n",
    "# archiver = ArchiverConnector(mode = \"production\")\n",
    "\n",
    "# Getting the project path\n",
    "project_path = config[\"PATH\"]\n",
    "\n",
    "# Loading the portal list\n",
    "portal_list = pd.read_csv(project_path + \"data_portal_tracker/data/portals.csv\")\n",
    "\n",
    "# TESTING ONLY: loading a subset of the portal list that covers a wide range of API versions\n",
    "portal_list_test = pd.read_csv(project_path + \"data_portal_tracker/data/portals_test_subset.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opendatasoft API v1.0\n",
    "This function was created for completeness and to perform some comparisons with v2.0 / v2.1 - Opendatasoft API v1.0 is now deprecated, use the function for v2.1 below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_opendatasoft_v1(portal_list: str, statistics_file: str):\n",
    "    \"\"\"Crawling all portals on the list that support the Opendatasoft API v1.0, inserting all datasets and metadata of each portal into the Archiver and saving statistics.\n",
    "\n",
    "    Args:\n",
    "        ``portal_list (str):`` the path of the CSV input file containing the final portal list - must be a file created previously by \"extract_working_apis()\" in the portal handler\n",
    "        \n",
    "        ``statistics_file (str):`` the path of the CSV file to be created or extended, containing the statistics for the crawled portals\n",
    "    \"\"\"\n",
    "\n",
    "    # Getting the current timestamp\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "    # Creating a dataframe to log failed API requests\n",
    "    failed_api_requests = pd.DataFrame([(None, None, None, None, None, None)], columns = [\"timestamp\", \"api_request_url\", \"dataset_url\", \"metadata_url\", \"source_url\", \"exception\"])\n",
    "    failed_api_requests_filename = project_path + \"data_portal_tracker/logs/crawl_opendatasoft_v1_\" + current_timestamp + \"_fail.csv\"\n",
    "\n",
    "    # Setting file paths for the log files to be created by handle_dataset()\n",
    "    log_file_success = project_path +  \"data_portal_tracker/logs/handle_dataset_opendatasoft_v1_\" + current_timestamp + \"_success.csv\"\n",
    "    log_file_fail = project_path + \"data_portal_tracker/logs/handle_dataset_opendatasoft_v1_\" + current_timestamp + \"_fail.csv\"\n",
    "\n",
    "    # Creating lists for the API base URLs and API method URLs\n",
    "    api_base_urls = []\n",
    "    api_method_urls = []\n",
    "\n",
    "    # Creating a list of portals that use Opendatasoft v1.0 and have a working API\n",
    "    for i in range(len(portal_list)):\n",
    "        if ((portal_list[\"api_software\"][i] == \"OpenDataSoft\") & (\"v1.0\" in portal_list[\"api_version\"][i]) & (portal_list[\"api_working\"][i] == True)):\n",
    "            api_base_url = portal_list[\"url\"][i]\n",
    "            api_base_urls.append(api_base_url)\n",
    "            api_method_urls.append(api_base_url + \"/api/datasets/1.0/search/?\")\n",
    "\n",
    "    # Printing information\n",
    "    print(\"Crawling portals supporting Opendatasoft v1.0\")\n",
    "\n",
    "    # Looping through the Opendatasoft v1.0 portals\n",
    "    for i in range(len(api_method_urls)):\n",
    "\n",
    "        # Creating a dataframe for the portal statistics\n",
    "        portal_statistics = pd.DataFrame([(None, None, None, None, None)], columns = [\"url\", \"api_software\", \"number_of_datasets\", \"number_of_supported_datasets\", \"timestamp\"])\n",
    "\n",
    "        # Setting the variable indicating if there are still unseen datasets on the portal\n",
    "        datasets_available = True\n",
    "\n",
    "        # Setting the number of datasets to be returned for each request\n",
    "        datasets_per_request = 800\n",
    "\n",
    "        # Setting the index of the first dataset to be returned\n",
    "        index_of_current_dataset = 0\n",
    "\n",
    "        # Resetting the number of datasets counted in the response to the current request\n",
    "        datasets_in_current_response = 0\n",
    "\n",
    "        # Resetting dataset, metadata and source variables for error logging purposes\n",
    "        dataset_url = None\n",
    "        metadata_url = None\n",
    "        source_url = None\n",
    "        metadata = None\n",
    "\n",
    "        # Setting the maximum number of attempts in case of an exception before skipping the portal\n",
    "        maximum_attempts = 3\n",
    "\n",
    "        # Setting the number of the current attempt\n",
    "        attempt_number = 1\n",
    "\n",
    "        # Printing the portal\n",
    "        print(\"\\n\" + \"Portal \" + str(i+1) + \"/\" + str(len(api_method_urls)) + \": \" + api_base_urls[i])\n",
    "\n",
    "        # Iterating over all datasets on the portal in batches of 800 until we run out of datasets\n",
    "        while datasets_available:\n",
    "            # Building the API request URL\n",
    "            api_request_url = remove_double_slashes(api_method_urls[i] + \"rows=\" + str(datasets_per_request) + \"&start=\" + str(index_of_current_dataset))\n",
    "\n",
    "            # Waiting before each request in order not to flood the API with requests\n",
    "            sleep(2)\n",
    "            \n",
    "            try:\n",
    "                # Making the API request and deserializing the JSON response string\n",
    "                response = json.loads(requests.get(api_request_url).text)\n",
    "\n",
    "                # Getting the total number of datasets on the portal\n",
    "                total_number_of_datasets = response[\"nhits\"]\n",
    "\n",
    "                # During the first iteration / request (so only once)\n",
    "                if(index_of_current_dataset == 0):\n",
    "                    # Printing the total number of datasets\n",
    "                    print(\"Total number of datasets: \" + str(total_number_of_datasets))\n",
    "\n",
    "                    # Saving information to the statistics dataframe\n",
    "                    portal_statistics.loc[0, \"url\"] = api_base_urls[i]\n",
    "                    portal_statistics.loc[0, \"api_software\"] = \"Opendatasoft\"\n",
    "                    portal_statistics.loc[0, \"number_of_datasets\"] = int(total_number_of_datasets)\n",
    "                    portal_statistics.loc[0, \"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                    # Export statistics to a CSV file\n",
    "                    portal_statistics.to_csv(statistics_file, mode = \"a\", index = False, header = not os.path.isfile(statistics_file))\n",
    "\n",
    "                # Printing the current API request URL\n",
    "                print(\"\\n\" + \"Currently crawling: \" + api_request_url + \"\\n\")\n",
    "\n",
    "                # Extracting the metadata from the current response\n",
    "                metadata = response[\"datasets\"]\n",
    "\n",
    "                # Counting the number of datasets in the metadata\n",
    "                datasets_in_current_response = len(metadata)\n",
    "                \n",
    "                # Iterating over all the metadata of the current response\n",
    "                for j in range(datasets_in_current_response):\n",
    "                    # Getting the ID of each dataset\n",
    "                    dataset_id = str(metadata[j][\"datasetid\"])\n",
    "\n",
    "                    # Building the metadata URL and dataset URL\n",
    "                    metadata_url = remove_double_slashes(api_base_urls[i] + \"/api/datasets/1.0/\") + dataset_id\n",
    "                    dataset_url = remove_double_slashes(api_base_urls[i] + \"/api/records/1.0/download?dataset=\") + dataset_id + \"&format=csv\"\n",
    "\n",
    "                    \"\"\"\n",
    "                    # Optional: If the portal is the Opendatasoft data hub, building the URL of the original data source (= different site than the data hub) using the metadata.\n",
    "                    if (search(\"https://data.opendatasoft.com\", api_base_urls[i])):\n",
    "                        # Starting from the second iteration, comparing the domain of the current and previous original source\n",
    "                        if j != 0:\n",
    "                            previous_domain = remove_double_slashes(metadata[j-1][\"metas\"][\"source_domain_address\"])\n",
    "                        else:\n",
    "                            previous_domain = None\n",
    "                        current_domain = remove_double_slashes(metadata[j][\"metas\"][\"source_domain_address\"])\n",
    "\n",
    "                        # Wait 2 seconds between two requests to the same portal\n",
    "                        if current_domain == previous_domain:\n",
    "                            sleep(2)\n",
    "                        \n",
    "                        # Checking the protocol, adding the protocol prefix and building the original source URL\n",
    "                        original_source_url = check_protocol(remove_double_slashes(metadata[j][\"metas\"][\"source_domain_address\"] + \"/explore/dataset/\") + metadata[j][\"metas\"][\"source_dataset\"], show_details = False)\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    # Building the source URL using the API base url\n",
    "                    source_url = remove_double_slashes(api_base_urls[i] + \"/explore/dataset/\") + dataset_id\n",
    "\n",
    "                    # Calling the Archiver connector to insert data into the Archiver\n",
    "                    archiver.handle_dataset(dataset_url, metadata_url, source_url, log_file_success, log_file_fail)\n",
    "\n",
    "                    # Printing dataset information\n",
    "                    print(\"Dataset \" + str(j + 1 + index_of_current_dataset) + \"/\" + str(total_number_of_datasets))\n",
    "                    # print(\"Dataset URL: \" + dataset_url)\n",
    "                    # print(\"Metadata URL: \" + metadata_url)\n",
    "                    # print(\"Source URL: \" + source_url + \"\\n\")\n",
    "                \n",
    "                # Setting the index of the next dataset to be returned\n",
    "                index_of_current_dataset += datasets_in_current_response \n",
    "\n",
    "                # Stopping the loop after this iteration if less than 800 datasets are returned (meaning that these are the last available datasets)\n",
    "                if datasets_in_current_response != datasets_per_request:\n",
    "                    datasets_available = False\n",
    "\n",
    "                # Stopping the loop after this iteration if the index of the next requested dataset would exceed the index of the last available dataset\n",
    "                if index_of_current_dataset >= total_number_of_datasets:\n",
    "                    datasets_available = False\n",
    "            except Exception as exception:\n",
    "                # Saving the failed API requests to the dataframe, then saving them to a new CSV file or appending them to an existing one\n",
    "                failed_api_requests.loc[0, \"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                failed_api_requests.loc[0, \"api_request_url\"] = api_request_url\n",
    "                failed_api_requests.loc[0, \"exception\"] = repr(exception)\n",
    "                try:\n",
    "                    failed_api_requests.loc[0, \"dataset_url\"] = dataset_url\n",
    "                    failed_api_requests.loc[0, \"metadata_url\"] = metadata_url\n",
    "                    failed_api_requests.loc[0, \"source_url\"] = source_url\n",
    "                except NameError:\n",
    "                    pass\n",
    "                failed_api_requests.to_csv(failed_api_requests_filename, mode = \"a\", index = False, header = not os.path.isfile(failed_api_requests_filename))\n",
    "                print(\"An exception occurred!\")\n",
    "\n",
    "                # If the maximum number of attempts has been reached, skipping the portal \n",
    "                if attempt_number == maximum_attempts:\n",
    "                    datasets_available = False\n",
    "                # Otherwise, increasing the number of attempts by 1\n",
    "                else:\n",
    "                    attempt_number += 1\n",
    "                    sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opendatasoft API v2.1\n",
    "The API responses of v2.1 differ slightly from v2.0 - however, since every portal in our list that supports v2.0 also supports v2.1, there is no need for a separate v2.0 function. This function takes roughly 2 seconds per dataset, which means it can work through 30 datasets per minute or 1800 datasets per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_opendatasoft_v2(portal_list: str, statistics_file: str):\n",
    "    \"\"\"Crawling all portals on the list that support the Opendatasoft API v2.1, inserting all datasets and metadata of each portal into the Archiver and saving statistics.\n",
    "\n",
    "    Args:\n",
    "        ``portal_list (str):`` the path of the CSV input file containing the final portal list - must be a file created previously by \"extract_working_apis()\" in the portal handler\n",
    "        \n",
    "        ``statistics_file (str):`` the path of the CSV file to be created or extended, containing the statistics for the crawled portals\n",
    "    \"\"\"\n",
    "        \n",
    "    # Getting the current timestamp\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "    # Creating a dataframe to log failed API requests\n",
    "    failed_api_requests = pd.DataFrame([(None, None, None, None, None, None)], columns = [\"timestamp\", \"api_request_url\", \"dataset_url\", \"metadata_url\", \"source_url\", \"exception\"])\n",
    "    failed_api_requests_filename = project_path + \"data_portal_tracker/logs/crawl_opendatasoft_v2_\" + current_timestamp + \"_fail.csv\"\n",
    "\n",
    "    # Setting file paths for the log files to be created by handle_dataset()\n",
    "    log_file_success = project_path +  \"data_portal_tracker/logs/handle_dataset_opendatasoft_v2_\" + current_timestamp + \"_success.csv\"\n",
    "    log_file_fail = project_path + \"data_portal_tracker/logs/handle_dataset_opendatasoft_v2_\" + current_timestamp + \"_fail.csv\"\n",
    "\n",
    "    # Creating lists for the API base URLs and API method URLs\n",
    "    api_base_urls = []\n",
    "    api_method_urls = []\n",
    "\n",
    "    # Creating a list of portals that use Opendatasoft v2.1 and have a working API\n",
    "    for i in range(len(portal_list)):\n",
    "        if ((portal_list[\"api_software\"][i] == \"OpenDataSoft\") & (\"v2.1\" in portal_list[\"api_version\"][i]) & (portal_list[\"api_working\"][i] == True)):\n",
    "            api_base_url = portal_list[\"url\"][i]\n",
    "            api_base_urls.append(api_base_url)\n",
    "            api_method_urls.append(api_base_url + \"/api/explore/v2.1\")\n",
    "\n",
    "    # Printing information\n",
    "    print(\"Crawling portals supporting Opendatasoft v2.1\")\n",
    "\n",
    "    # Looping through the Opendatasoft v2.1 portals\n",
    "    for i in range(len(api_method_urls)):\n",
    "\n",
    "        # Creating a dataframe for the portal statistics\n",
    "        portal_statistics = pd.DataFrame([(None, None, None, None, None)], columns = [\"url\", \"api_software\", \"number_of_datasets\", \"number_of_supported_datasets\", \"timestamp\"])\n",
    "\n",
    "        # Resetting dataset, metadata and source variables for error logging purposes\n",
    "        dataset_url = None\n",
    "        metadata_url = None\n",
    "        source_url = None\n",
    "        metadata = None\n",
    "\n",
    "        # Setting the variable counting the number of supported datasets on the portal\n",
    "        number_of_supported_datasets = 0\n",
    "        \n",
    "        # Setting the maximum number of attempts in case of an exception before skipping the portal\n",
    "        maximum_attempts = 3\n",
    "\n",
    "        # Setting the number of the current attempt\n",
    "        attempt_number = 1\n",
    "\n",
    "        # Setting the error variable\n",
    "        error = False\n",
    "\n",
    "        # Setting the variable that indicates whether the portal's catalog has been downloaded\n",
    "        catalog_exported = False\n",
    "\n",
    "        # Printing the portal\n",
    "        print(\"\\n\" + \"Portal \" + str(i+1) + \"/\" + str(len(api_method_urls)) + \": \" + api_base_urls[i])\n",
    "\n",
    "        # Looping as long as the catalog has not been downloaded\n",
    "        while not catalog_exported:\n",
    "            # Building the API request URL\n",
    "            api_request_url = remove_double_slashes(api_method_urls[i] + \"/catalog/exports/json\")\n",
    "\n",
    "            try:\n",
    "                # Making the API request and deserializing the JSON response string\n",
    "                metadata = json.loads(requests.get(api_request_url).text)\n",
    "\n",
    "                # Indicating successful export\n",
    "                catalog_exported = True\n",
    "\n",
    "                # Getting the total number of datasets on the portal\n",
    "                total_number_of_datasets = len(metadata)\n",
    "\n",
    "                # Comment this step out if the optional code below is used!\n",
    "                number_of_supported_datasets = total_number_of_datasets\n",
    "\n",
    "                # Printing the catalog API export URL\n",
    "                print(\"\\n\" + \"Currently crawling: \" + api_request_url + \"\\n\")\n",
    "                \n",
    "                # Iterating over all the metadata of the response\n",
    "                for j in range(len(metadata)):\n",
    "                    # Getting the ID of each dataset\n",
    "                    dataset_id = str(metadata[j][\"dataset_id\"])\n",
    "\n",
    "                    \"\"\"\n",
    "                    # Optional: Checking the available export formats of the dataset\n",
    "\n",
    "                    # Waiting before each request in order not to flood the API with requests\n",
    "                    # sleep(1)\n",
    "\n",
    "                    dataset_formats_url = remove_double_slashes(api_base_urls[i] + \"/api/explore/v2.1/catalog/datasets/\") + dataset_id + \"/exports\"\n",
    "                    dataset_formats = json.loads(requests.get(dataset_formats_url).text)\n",
    "\n",
    "                    # Choosing CSV if available, else JSON (this list could be extended, check the URL above for options!)\n",
    "                    for link in dataset_formats[\"links\"]:\n",
    "                        if \"csv\" in link.values():\n",
    "                            dataset_format = \"csv\"\n",
    "                            break\n",
    "                        elif \"json\" in link.values():\n",
    "                            dataset_format = \"json\"\n",
    "                            break\n",
    "\n",
    "                    # Increasing the number of supported datasets if the dataset is available in one of the specified formats, else skipping the dataset\n",
    "                    if dataset_format in [\"csv\", \"json\"]:\n",
    "                        number_of_supported_datasets += 1\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "                    # If the optional code is used:\n",
    "                        # Swap the used dataset_url line below (comment / uncomment)\n",
    "                        # Comment out the \"number_of_supported_datasets = total_number_of_datasets\" step above\n",
    "                    \"\"\"\n",
    "\n",
    "                    # Building the metadata URL, dataset URL and source URL\n",
    "                    metadata_url = remove_double_slashes(api_base_urls[i] + \"/api/explore/v2.1/catalog/datasets/\") + dataset_id\n",
    "                    dataset_url = remove_double_slashes(api_base_urls[i] + \"/api/explore/v2.1/catalog/datasets/\") + dataset_id + \"/exports/\" + \"csv\"\n",
    "                    # dataset_url = remove_double_slashes(api_base_urls[i] + \"/api/explore/v2.1/catalog/datasets/\") + dataset_id + \"/exports/\" + dataset_format\n",
    "                    source_url = remove_double_slashes(api_base_urls[i] + \"/explore/dataset/\") + dataset_id\n",
    "\n",
    "                    # Calling the Archiver connector to insert data into the Archiver\n",
    "                    archiver.handle_dataset(dataset_url, metadata_url, source_url, log_file_success, log_file_fail)\n",
    "\n",
    "                    # Printing dataset information\n",
    "                    print(\"Dataset \" + str(j + 1) + \"/\" + str(total_number_of_datasets))\n",
    "                    # print(\"Dataset URL: \" + dataset_url)\n",
    "                    # print(\"Metadata URL: \" + metadata_url)\n",
    "                    # print(\"Source URL: \" + source_url + \"\\n\")\n",
    "                \n",
    "            except Exception as exception:\n",
    "                # Saving the failed API requests to the dataframe, then saving them to a new CSV file or appending them to an existing one\n",
    "                failed_api_requests.loc[0, \"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                failed_api_requests.loc[0, \"api_request_url\"] = api_request_url\n",
    "                failed_api_requests.loc[0, \"exception\"] = repr(exception)\n",
    "                try:\n",
    "                    failed_api_requests.loc[0, \"dataset_url\"] = dataset_url\n",
    "                    failed_api_requests.loc[0, \"metadata_url\"] = metadata_url\n",
    "                    failed_api_requests.loc[0, \"source_url\"] = source_url\n",
    "                except NameError:\n",
    "                    pass\n",
    "                failed_api_requests.to_csv(failed_api_requests_filename, mode = \"a\", index = False, header = not os.path.isfile(failed_api_requests_filename))\n",
    "                print(\"An exception occurred!\")\n",
    "\n",
    "                # If the maximum number of attempts has been reached, skipping the portal \n",
    "                if attempt_number == maximum_attempts:\n",
    "                    catalog_exported = True\n",
    "                    error = True\n",
    "                # Otherwise, increasing the number of attempts by 1\n",
    "                else:\n",
    "                    attempt_number += 1\n",
    "                    sleep(2)\n",
    "\n",
    "        # Printing the number of (total/supported) datasets\n",
    "        print(\"\\n\" + \"Total number of datasets on \" + api_base_urls[i] + \" : \" + str(total_number_of_datasets))\n",
    "        print(\"Number of supported datasets on \" + api_base_urls[i] + \" : \" + str(number_of_supported_datasets) + \"\\n\")\n",
    "\n",
    "        # Saving information to the statistics dataframe\n",
    "        portal_statistics.loc[0, \"url\"] = api_base_urls[i]\n",
    "        portal_statistics.loc[0, \"api_software\"] = \"Opendatasoft\"\n",
    "        if error is False:\n",
    "            portal_statistics.loc[0, \"number_of_datasets\"] = int(total_number_of_datasets)\n",
    "            portal_statistics.loc[0, \"number_of_supported_datasets\"] = int(number_of_supported_datasets)\n",
    "        portal_statistics.loc[0, \"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Export statistics to a CSV file\n",
    "        portal_statistics.to_csv(statistics_file, mode = \"a\", index = False, header = not os.path.isfile(statistics_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKAN API v2.x\n",
    "The function below has been tested with portals using a wide range of CKAN API versions, ranging from v2.0 to v2.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_ckan(portal_list: str, statistics_file: str):\n",
    "    \"\"\"Crawling all portals on the list that support the CKAN API v2.x, inserting all datasets (CKAN term: resources) and metadata of each portal into the Archiver and saving statistics.\n",
    "\n",
    "    Args:\n",
    "        ``portal_list (str):`` the path of the CSV input file containing the final portal list - must be a file created previously by \"extract_working_apis()\" in the portal handler\n",
    "        \n",
    "        ``statistics_file (str):`` the path of the CSV file to be created or extended, containing the statistics for the crawled portals\n",
    "    \"\"\"\n",
    "\n",
    "    # Getting the current timestamp\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "    # Creating a dataframe to log failed API requests\n",
    "    failed_api_requests = pd.DataFrame([(None, None, None, None, None, None)], columns = [\"timestamp\", \"api_request_url\", \"resource_url\", \"metadata_url\", \"source_url\", \"exception\"])\n",
    "    failed_api_requests_filename = project_path + \"data_portal_tracker/logs/crawl_ckan_\" + current_timestamp + \"_fail.csv\"\n",
    "\n",
    "    # Setting file paths for the log files to be created by handle_dataset()\n",
    "    log_file_success = project_path +  \"data_portal_tracker/logs/handle_dataset_ckan_\" + current_timestamp + \"_success.csv\"\n",
    "    log_file_fail = project_path + \"data_portal_tracker/logs/handle_dataset_ckan_\" + current_timestamp + \"_fail.csv\"\n",
    "\n",
    "    # Creating lists for the API base URLs and API method URLs\n",
    "    api_base_urls = []\n",
    "    api_method_urls = []\n",
    "\n",
    "    # Creating a list of portals that use CKAN and have a working API\n",
    "    for i in range(len(portal_list)):\n",
    "        if ((portal_list[\"api_software\"][i] == \"CKAN\") & (portal_list[\"api_working\"][i] == True)):\n",
    "            api_base_url = portal_list[\"url\"][i]\n",
    "            api_base_urls.append(api_base_url)\n",
    "            api_method_urls.append(api_base_url + \"/api/3/action/package_search?\")\n",
    "\n",
    "    # Printing information\n",
    "    print(\"Crawling portals supporting CKAN\")\n",
    "    print(\"On CKAN portals, one dataset/package can contain multiple resources.\")\n",
    "    print('The term \"resource\" on CKAN portals is mostly equivalent to the term \"dataset\" used in this project.')\n",
    "\n",
    "    # Looping through the CKAN portals\n",
    "    for i in range(len(api_method_urls)):\n",
    "\n",
    "        # Creating a dataframe for the portal statistics\n",
    "        portal_statistics = pd.DataFrame([(None, None, None, None, None)], columns = [\"url\", \"api_software\", \"number_of_datasets\", \"number_of_resources\", \"timestamp\"])\n",
    "\n",
    "        # Setting the variable indicating if there are still unseen datasets on the portal\n",
    "        datasets_available = True\n",
    "\n",
    "        # Setting the number of datasets to be returned for each request\n",
    "        datasets_per_request = 800\n",
    "\n",
    "        # Setting the index of the first dataset to be returned\n",
    "        index_of_current_dataset = 0\n",
    "\n",
    "        # Resetting the number of datasets counted in the response to the current request\n",
    "        datasets_in_current_response = 0\n",
    "\n",
    "        # Setting the variables counting the total number of datasets and resources on the portal\n",
    "        total_number_of_datasets = 0\n",
    "        total_number_of_resources = 0\n",
    "\n",
    "        # Resetting resource, metadata and source variables for error logging purposes\n",
    "        resource_url = None\n",
    "        metadata_url = None\n",
    "        source_url = None\n",
    "        metadata = None\n",
    "\n",
    "        # Setting the maximum number of attempts in case of an exception before skipping the portal\n",
    "        maximum_attempts = 3\n",
    "\n",
    "        # Setting the number of the current attempt\n",
    "        attempt_number = 1\n",
    "\n",
    "        # Setting the error variable\n",
    "        error = False\n",
    "\n",
    "        # Printing the portal\n",
    "        print(\"\\n\" + \"Portal \" + str(i+1) + \"/\" + str(len(api_method_urls)) + \": \" + api_base_urls[i])\n",
    "\n",
    "        # Iterating over all datasets on the portal in batches of 800 until we run out of datasets\n",
    "        while datasets_available:\n",
    "            # Building the API request URL\n",
    "            api_request_url = remove_double_slashes(api_method_urls[i] + \"rows=\" + str(datasets_per_request) + \"&start=\" + str(index_of_current_dataset))\n",
    "\n",
    "            # Waiting before each request in order not to flood the API with requests\n",
    "            sleep(2)\n",
    "            \n",
    "            try:\n",
    "                # Making the API request and deserializing the JSON response string\n",
    "                response = json.loads(requests.get(api_request_url).text)\n",
    "\n",
    "                # Getting the total number of datasets on the portal\n",
    "                total_number_of_datasets = response[\"result\"][\"count\"]\n",
    "\n",
    "                # During the first iteration / request (so only once)\n",
    "                if(index_of_current_dataset == 0):\n",
    "                    # Printing the total number of datasets\n",
    "                    print(\"Total number of datasets: \" + str(total_number_of_datasets))\n",
    "\n",
    "                # Printing the current API request URL\n",
    "                print(\"\\n\" + \"Currently crawling: \" + api_request_url)\n",
    "\n",
    "                # Extracting the metadata from the current response\n",
    "                metadata = response[\"result\"][\"results\"]\n",
    "\n",
    "                # Counting the number of datasets in the metadata\n",
    "                datasets_in_current_response = len(metadata)\n",
    "                \n",
    "                # Iterating over all the metadata of the current response\n",
    "                for j in range(datasets_in_current_response):\n",
    "\n",
    "                    # Printing the current dataset number\n",
    "                    print(\"\\n\" + \"Dataset \" + str(j + 1 + index_of_current_dataset) + \"/\" + str(total_number_of_datasets))\n",
    "\n",
    "                    # Getting and printing the number of resources\n",
    "                    number_of_resources = metadata[j][\"num_resources\"]\n",
    "                    print(\"Number of resources: \" + str(number_of_resources) + \"\\n\")\n",
    "\n",
    "                    # Adding the number to the total resource number\n",
    "                    total_number_of_resources += number_of_resources\n",
    "\n",
    "                    # Iterating over all of the resources of a dataset, if any\n",
    "                    if number_of_resources != 0:\n",
    "                        for k, resource in enumerate(metadata[j][\"resources\"]):\n",
    "                            # Getting the dataset ID\n",
    "                            dataset_id = metadata[j][\"id\"]\n",
    "\n",
    "                            # Getting the resource URL\n",
    "                            resource_url = resource[\"url\"]\n",
    "\n",
    "                            # Building the metadata URL (we are using the metadata of the dataset/package!)\n",
    "                            metadata_url = remove_double_slashes(api_base_urls[i] + \"/api/3/action/package_show?id=\") + dataset_id\n",
    " \n",
    "                            # Building the source URL (we are using the source of the dataset/package!)\n",
    "                            source_url = remove_double_slashes(api_base_urls[i] + \"/dataset/\") + dataset_id\n",
    "\n",
    "                            # Calling the Archiver connector to insert data into the Archiver\n",
    "                            archiver.handle_dataset(resource_url, metadata_url, source_url, log_file_success, log_file_fail)\n",
    "\n",
    "                            # Printing resource information\n",
    "                            print(\"Resource \" + str(k + 1) + \"/\" + str(number_of_resources))\n",
    "                            # print(\"Resource URL: \" + resource_url)\n",
    "                            # print(\"Metadata URL: \" + metadata_url)\n",
    "                            # print(\"Source URL: \" + source_url + \"\\n\")\n",
    "                    \n",
    "                # Setting the index of the next dataset to be returned\n",
    "                index_of_current_dataset += datasets_in_current_response \n",
    "\n",
    "                # Stopping the loop after this iteration if less than 800 datasets are returned (meaning that these are the last available datasets)\n",
    "                if datasets_in_current_response != datasets_per_request:\n",
    "                    datasets_available = False\n",
    "\n",
    "                # Stopping the loop after this iteration if the index of the next requested dataset would exceed the index of the last available dataset\n",
    "                if index_of_current_dataset >= total_number_of_datasets:\n",
    "                    datasets_available = False\n",
    "            except Exception as exception:\n",
    "                # Saving the failed API requests to the dataframe, then saving them to a new CSV file or appending them to an existing one\n",
    "                failed_api_requests.loc[0, \"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                failed_api_requests.loc[0, \"api_request_url\"] = api_request_url\n",
    "                failed_api_requests.loc[0, \"exception\"] = repr(exception)\n",
    "                try:\n",
    "                    failed_api_requests.loc[0, \"resource_url\"] = resource_url\n",
    "                    failed_api_requests.loc[0, \"metadata_url\"] = metadata_url\n",
    "                    failed_api_requests.loc[0, \"source_url\"] = source_url\n",
    "                except NameError:\n",
    "                    pass\n",
    "                failed_api_requests.to_csv(failed_api_requests_filename, mode = \"a\", index = False, header = not os.path.isfile(failed_api_requests_filename))\n",
    "                print(\"An exception occurred!\")\n",
    "\n",
    "                # If the maximum number of attempts has been reached, skipping the portal \n",
    "                if attempt_number == maximum_attempts:\n",
    "                    datasets_available = False\n",
    "                    error = True\n",
    "                # Otherwise, increasing the number of attempts by 1\n",
    "                else:\n",
    "                    attempt_number += 1\n",
    "                    sleep(2)\n",
    "        \n",
    "        # Printing the total number of resources\n",
    "        print(\"\\n\" + \"Total number of resources on \" + api_base_urls[i] + \" : \" + str(total_number_of_resources) + \"\\n\")\n",
    "\n",
    "        # Saving information to the statistics dataframe\n",
    "        portal_statistics.loc[0, \"url\"] = api_base_urls[i]\n",
    "        portal_statistics.loc[0, \"api_software\"] = \"CKAN\"\n",
    "        if error is False:\n",
    "            portal_statistics.loc[0, \"number_of_datasets\"] = int(total_number_of_datasets)\n",
    "            portal_statistics.loc[0, \"number_of_resources\"] = int(total_number_of_resources)\n",
    "        portal_statistics.loc[0, \"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Export statistics to a CSV file\n",
    "        portal_statistics.to_csv(statistics_file, mode = \"a\", index = False, header = not os.path.isfile(statistics_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socrata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_socrata(portal_list: str, statistics_file: str):\n",
    "    \"\"\"Crawling all portals on the list that support the Socrata API, inserting all datasets and metadata of each portal into the Archiver and saving statistics.\n",
    "\n",
    "    Args:\n",
    "        ``portal_list (str):`` the path of the CSV input file containing the final portal list - must be a file created previously by \"extract_working_apis()\" in the portal handler\n",
    "        \n",
    "        ``statistics_file (str):`` the path of the CSV file to be created or extended, containing the statistics for the crawled portals\n",
    "    \"\"\"\n",
    "\n",
    "    # Getting the current timestamp\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "    # Creating a dataframe to log failed API requests\n",
    "    failed_api_requests = pd.DataFrame([(None, None, None, None, None, None)], columns = [\"timestamp\", \"api_request_url\", \"dataset_url\", \"metadata_url\", \"source_url\", \"exception\"])\n",
    "    failed_api_requests_filename = project_path + \"data_portal_tracker/logs/crawl_socrata_\" + current_timestamp + \"_fail.csv\"\n",
    "\n",
    "    # Setting file paths for the log files to be created by handle_dataset()\n",
    "    log_file_success = project_path +  \"data_portal_tracker/logs/handle_dataset_socrata_\" + current_timestamp + \"_success.csv\"\n",
    "    log_file_fail = project_path + \"data_portal_tracker/logs/handle_dataset_socrata_\" + current_timestamp + \"_fail.csv\"\n",
    "\n",
    "    # Creating lists for the API base URLs and API method URLs\n",
    "    api_base_urls = []\n",
    "    api_method_urls = []\n",
    "\n",
    "    # Creating a list of portals that use Socrata and have a working API\n",
    "    for i in range(len(portal_list)):\n",
    "        if ((portal_list[\"api_software\"][i] == \"Socrata\") & (portal_list[\"api_working\"][i] == True)):\n",
    "            api_base_url = portal_list[\"url\"][i]\n",
    "            api_base_urls.append(api_base_url)\n",
    "            api_method_urls.append(api_base_url + \"/api/views\")\n",
    "\n",
    "    # Printing information\n",
    "    print(\"Crawling portals supporting Socrata\")\n",
    "\n",
    "    # Looping through the Socrata portals\n",
    "    for i in range(len(api_method_urls)):\n",
    "\n",
    "        # Creating a dataframe for the portal statistics\n",
    "        portal_statistics = pd.DataFrame([(None, None, None, None, None)], columns = [\"url\", \"api_software\", \"number_of_datasets\", \"number_of_supported_datasets\", \"timestamp\"])\n",
    "\n",
    "        # Setting the variable indicating if there are still unseen datasets on the portal\n",
    "        datasets_available = True\n",
    "\n",
    "        # Setting the number of datasets to be returned for each request\n",
    "        datasets_per_request = 800\n",
    "\n",
    "        # Setting the number of the first page to be requested (Socrata pagination starts with 1)\n",
    "        current_page = 1\n",
    "\n",
    "        # Setting the variables counting the number of (total/supported) datasets on the portal\n",
    "        total_number_of_datasets = 0\n",
    "        number_of_supported_datasets = 0\n",
    "\n",
    "        # Resetting the number of datasets counted in the response to the current request\n",
    "        datasets_in_current_response = 0\n",
    "\n",
    "        # Resetting dataset, metadata and source variables for error logging purposes\n",
    "        dataset_url = None\n",
    "        metadata_url = None\n",
    "        source_url = None\n",
    "        metadata = None\n",
    "\n",
    "        # Setting the maximum number of attempts in case of an exception before skipping the portal\n",
    "        maximum_attempts = 3\n",
    "\n",
    "        # Setting the number of the current attempt\n",
    "        attempt_number = 1\n",
    "\n",
    "        # Setting the error variable\n",
    "        error = False\n",
    "\n",
    "        # Printing the portal\n",
    "        print(\"\\n\" + \"Portal \" + str(i+1) + \"/\" + str(len(api_method_urls)) + \": \" + api_base_urls[i])\n",
    "\n",
    "        # Iterating over all datasets on the portal in batches of 800 until we run out of datasets\n",
    "        while datasets_available:\n",
    "            # Building the API request URL\n",
    "            api_request_url = remove_double_slashes(api_method_urls[i] + \"?limit=\" + str(datasets_per_request) + \"&page=\" + str(current_page))\n",
    "\n",
    "            # Waiting before each request in order not to flood the API with requests\n",
    "            sleep(2)\n",
    "\n",
    "            # Printing the current API request URL\n",
    "            print(\"\\n\" + \"Currently crawling: \" + api_request_url + \"\\n\")\n",
    "            \n",
    "            try:\n",
    "                # Making the API request and deserializing the JSON response string\n",
    "                metadata = json.loads(requests.get(api_request_url).text)\n",
    "\n",
    "                # Stopping the loop if a page after the first one is empty (no datasets are available anymore)\n",
    "                if current_page > 1 and metadata == []:\n",
    "                    datasets_available = False\n",
    "                    break\n",
    "\n",
    "                # Counting the number of datasets in the metadata\n",
    "                datasets_in_current_response = len(metadata)\n",
    "\n",
    "                # Iterating over all the metadata of the current response\n",
    "                for j in range(datasets_in_current_response):\n",
    "\n",
    "                    # Printing the current dataset number\n",
    "                    print(\"Dataset \" + str(j + 1 + total_number_of_datasets))\n",
    "\n",
    "                    # Getting the dataset ID and asset type\n",
    "                    dataset_id = metadata[j][\"id\"]\n",
    "                    asset_type = metadata[j][\"assetType\"]\n",
    "\n",
    "                    # Getting the dataset URL if the asset type is supported, otherwise skipping the dataset\n",
    "                    if asset_type == \"dataset\":\n",
    "                        dataset_url = remove_double_slashes(api_method_urls[i] + \"/\") + dataset_id + \"/rows.csv?accessType=DOWNLOAD\"\n",
    "                    # After TESTING if the asset types \"chart\", \"datalens\", \"filter\" also always work with the method above, replace the if-statement above with ---> if asset_type == \"dataset\" or asset_type == \"chart\" or asset_type == \"datalens\" or asset_type == \"filter\": <---\n",
    "                    elif asset_type == \"file\":\n",
    "                        dataset_url = remove_double_slashes(api_base_urls[i] + \"/download/\") + dataset_id\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    # Increasing the number of supported datasets\n",
    "                    number_of_supported_datasets += 1\n",
    "\n",
    "                    # Building the metadata URL\n",
    "                    metadata_url = remove_double_slashes(api_method_urls[i] + \"/metadata/v1/\") + dataset_id\n",
    "\n",
    "                    # Building the source URL\n",
    "                    source_url = remove_double_slashes(api_base_urls[i] + \"/d/\") + dataset_id\n",
    "\n",
    "                    # Calling the Archiver connector to insert data into the Archiver\n",
    "                    archiver.handle_dataset(dataset_url, metadata_url, source_url, log_file_success, log_file_fail)\n",
    "\n",
    "                    # Printing dataset information\n",
    "                    # print(\"Dataset URL: \" + dataset_url)\n",
    "                    # print(\"Metadata URL: \" + metadata_url)\n",
    "                    # print(\"Source URL: \" + source_url + \"\\n\")\n",
    "                    \n",
    "                # Increasing the number of the page to be requested\n",
    "                current_page += 1\n",
    "\n",
    "                # Adding the current datasets to the total dataset number\n",
    "                total_number_of_datasets += datasets_in_current_response \n",
    "\n",
    "                # Stopping the loop after this iteration if less than 800 datasets are returned (meaning that these are the last available datasets)\n",
    "                if datasets_in_current_response != datasets_per_request:\n",
    "                    datasets_available = False\n",
    "            except Exception as exception:\n",
    "                # Saving the failed API requests to the dataframe, then saving them to a new CSV file or appending them to an existing one\n",
    "                failed_api_requests.loc[0, \"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                failed_api_requests.loc[0, \"api_request_url\"] = api_request_url\n",
    "                failed_api_requests.loc[0, \"exception\"] = repr(exception)\n",
    "                try:\n",
    "                    failed_api_requests.loc[0, \"dataset_url\"] = dataset_url\n",
    "                    failed_api_requests.loc[0, \"metadata_url\"] = metadata_url\n",
    "                    failed_api_requests.loc[0, \"source_url\"] = source_url\n",
    "                except NameError:\n",
    "                    pass\n",
    "                failed_api_requests.to_csv(failed_api_requests_filename, mode = \"a\", index = False, header = not os.path.isfile(failed_api_requests_filename))\n",
    "                print(\"An exception occurred!\")\n",
    "\n",
    "                # If the maximum number of attempts has been reached, skipping the portal \n",
    "                if attempt_number == maximum_attempts:\n",
    "                    datasets_available = False\n",
    "                    error = True\n",
    "                # Otherwise, increasing the number of attempts by 1\n",
    "                else:\n",
    "                    attempt_number += 1\n",
    "                    sleep(2)\n",
    "        \n",
    "        # Printing the number of (total/supported) datasets\n",
    "        print(\"\\n\" + \"Total number of datasets on \" + api_base_urls[i] + \" : \" + str(total_number_of_datasets))\n",
    "        print(\"Number of supported datasets on \" + api_base_urls[i] + \" : \" + str(number_of_supported_datasets) + \"\\n\")\n",
    "\n",
    "        # Saving information to the statistics dataframe\n",
    "        portal_statistics.loc[0, \"url\"] = api_base_urls[i]\n",
    "        portal_statistics.loc[0, \"api_software\"] = \"Socrata\"\n",
    "        if error is False:\n",
    "            portal_statistics.loc[0, \"number_of_datasets\"] = int(total_number_of_datasets)\n",
    "            portal_statistics.loc[0, \"number_of_supported_datasets\"] = int(number_of_supported_datasets)\n",
    "        portal_statistics.loc[0, \"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Export statistics to a CSV file\n",
    "        portal_statistics.to_csv(statistics_file, mode = \"a\", index = False, header = not os.path.isfile(statistics_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the functions\n",
    "\"\"\"\n",
    "crawl_opendatasoft_v1(portal_list = portal_list, \n",
    "                      statistics_file = \"data/portal_statistics_opendatasoft_test.csv\")\n",
    "\n",
    "crawl_opendatasoft_v2(portal_list = portal_list, \n",
    "                      statistics_file = \"data/portal_statistics_opendatasoft.csv\")\n",
    "\n",
    "crawl_ckan(portal_list = portal_list, \n",
    "           statistics_file = \"data/portal_statistics_ckan.csv\")\n",
    "           \n",
    "crawl_socrata(portal_list = portal_list, \n",
    "              statistics_file = \"data/portal_statistics_socrata.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For command line usage, navigate to the script path and execute:\n",
    "\n",
    "```bash\n",
    "python3 -c 'from portal_crawler import *; crawl_opendatasoft_v1(portal_list, \"data/portal_statistics_opendatasoft.csv\")'\n",
    "```\n",
    "\n",
    "```bash\n",
    "python3 -c 'from portal_crawler import *; crawl_opendatasoft_v2(portal_list, \"data/portal_statistics_opendatasoft.csv\")'\n",
    "```\n",
    "\n",
    "```bash\n",
    "python3 -c 'from portal_crawler import *; crawl_ckan(portal_list, \"data/portal_statistics_ckan.csv\")'\n",
    "```\n",
    "\n",
    "```bash\n",
    "python3 -c 'from portal_crawler import *; crawl_socrata(portal_list, \"data/portal_statistics_socrata.csv\")'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The quickest way of obtaining the numbers of datasets/resources per portal type is to run the functions crawl_opendatasoft_v2, crawl_ckan and crawl_socrata after commenting out the code that handles the datasets and adds them to the Archiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of checked Opendatasoft portals: 349\n",
      "Total number of datasets on Opendatasoft portals: 90,636\n",
      "Average number of datasets on Opendatasoft portals: 260\n",
      "Total number of supported datasets on Opendatasoft portals: 90,636\n",
      "Average number of supported datasets on Opendatasoft portals: 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>api_software</th>\n",
       "      <th>number_of_datasets</th>\n",
       "      <th>number_of_supported_datasets</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://data.opendatasoft.com</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>31507</td>\n",
       "      <td>31507</td>\n",
       "      <td>2023-08-10 21:43:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>https://smartregionidf.opendatasoft.com</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>8777</td>\n",
       "      <td>8777</td>\n",
       "      <td>2023-08-10 21:55:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://data.laregion.fr</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>1601</td>\n",
       "      <td>1601</td>\n",
       "      <td>2023-08-10 21:40:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>https://occitanie.opendatasoft.com</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>1601</td>\n",
       "      <td>1601</td>\n",
       "      <td>2023-08-10 21:50:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://analyzejerseycity.opendatasoft.com</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>1256</td>\n",
       "      <td>1256</td>\n",
       "      <td>2023-08-10 21:36:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            url  api_software  \\\n",
       "114               https://data.opendatasoft.com  Opendatasoft   \n",
       "307     https://smartregionidf.opendatasoft.com  Opendatasoft   \n",
       "91                     https://data.laregion.fr  Opendatasoft   \n",
       "227          https://occitanie.opendatasoft.com  Opendatasoft   \n",
       "3    https://analyzejerseycity.opendatasoft.com  Opendatasoft   \n",
       "\n",
       "     number_of_datasets  number_of_supported_datasets            timestamp  \n",
       "114               31507                         31507  2023-08-10 21:43:06  \n",
       "307                8777                          8777  2023-08-10 21:55:29  \n",
       "91                 1601                          1601  2023-08-10 21:40:51  \n",
       "227                1601                          1601  2023-08-10 21:50:33  \n",
       "3                  1256                          1256  2023-08-10 21:36:10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>api_software</th>\n",
       "      <th>number_of_datasets</th>\n",
       "      <th>number_of_supported_datasets</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://data.opendatasoft.com</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>31507</td>\n",
       "      <td>31507</td>\n",
       "      <td>2023-08-10 21:43:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>https://smartregionidf.opendatasoft.com</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>8777</td>\n",
       "      <td>8777</td>\n",
       "      <td>2023-08-10 21:55:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://data.laregion.fr</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>1601</td>\n",
       "      <td>1601</td>\n",
       "      <td>2023-08-10 21:40:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>https://occitanie.opendatasoft.com</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>1601</td>\n",
       "      <td>1601</td>\n",
       "      <td>2023-08-10 21:50:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://analyzejerseycity.opendatasoft.com</td>\n",
       "      <td>Opendatasoft</td>\n",
       "      <td>1256</td>\n",
       "      <td>1256</td>\n",
       "      <td>2023-08-10 21:36:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            url  api_software  \\\n",
       "114               https://data.opendatasoft.com  Opendatasoft   \n",
       "307     https://smartregionidf.opendatasoft.com  Opendatasoft   \n",
       "91                     https://data.laregion.fr  Opendatasoft   \n",
       "227          https://occitanie.opendatasoft.com  Opendatasoft   \n",
       "3    https://analyzejerseycity.opendatasoft.com  Opendatasoft   \n",
       "\n",
       "     number_of_datasets  number_of_supported_datasets            timestamp  \n",
       "114               31507                         31507  2023-08-10 21:43:06  \n",
       "307                8777                          8777  2023-08-10 21:55:29  \n",
       "91                 1601                          1601  2023-08-10 21:40:51  \n",
       "227                1601                          1601  2023-08-10 21:50:33  \n",
       "3                  1256                          1256  2023-08-10 21:36:10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of checked CKAN portals: 218\n",
      "Total number of datasets on CKAN portals: 2,729,379\n",
      "Average number of datasets on CKAN portals: 12,520\n",
      "Total number of resources on CKAN portals: 6,680,682\n",
      "Average number of resources on CKAN portals: 30,645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>api_software</th>\n",
       "      <th>number_of_datasets</th>\n",
       "      <th>number_of_resources</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://b2find.eudat.eu</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>1276193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-08-13 20:55:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://data.amerigeoss.org</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>647468.0</td>\n",
       "      <td>3064339.0</td>\n",
       "      <td>2023-08-14 00:19:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://catalog.data.gov</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>250615.0</td>\n",
       "      <td>1554272.0</td>\n",
       "      <td>2023-08-13 21:24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://dados.tce.rs.gov.br</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>60668.0</td>\n",
       "      <td>158403.0</td>\n",
       "      <td>2023-08-13 21:47:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://ckan.publishing.service.gov.uk</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>56145.0</td>\n",
       "      <td>213544.0</td>\n",
       "      <td>2023-08-13 21:31:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       url api_software  number_of_datasets  \\\n",
       "47                 https://b2find.eudat.eu         CKAN           1276193.0   \n",
       "81             https://data.amerigeoss.org         CKAN            647468.0   \n",
       "50                https://catalog.data.gov         CKAN            250615.0   \n",
       "76             https://dados.tce.rs.gov.br         CKAN             60668.0   \n",
       "66  https://ckan.publishing.service.gov.uk         CKAN             56145.0   \n",
       "\n",
       "    number_of_resources            timestamp  \n",
       "47                  0.0  2023-08-13 20:55:21  \n",
       "81            3064339.0  2023-08-14 00:19:50  \n",
       "50            1554272.0  2023-08-13 21:24:06  \n",
       "76             158403.0  2023-08-13 21:47:03  \n",
       "66             213544.0  2023-08-13 21:31:36  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>api_software</th>\n",
       "      <th>number_of_datasets</th>\n",
       "      <th>number_of_resources</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://data.amerigeoss.org</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>647468.0</td>\n",
       "      <td>3064339.0</td>\n",
       "      <td>2023-08-14 00:19:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://catalog.data.gov</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>250615.0</td>\n",
       "      <td>1554272.0</td>\n",
       "      <td>2023-08-13 21:24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>https://data.gov.ua</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>29243.0</td>\n",
       "      <td>225757.0</td>\n",
       "      <td>2023-08-14 00:34:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://ckan.publishing.service.gov.uk</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>56145.0</td>\n",
       "      <td>213544.0</td>\n",
       "      <td>2023-08-13 21:31:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>https://scmb-ckan-dev.research.dc.uq.edu.au</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>1343.0</td>\n",
       "      <td>182581.0</td>\n",
       "      <td>2023-08-14 01:15:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             url api_software  \\\n",
       "81                   https://data.amerigeoss.org         CKAN   \n",
       "50                      https://catalog.data.gov         CKAN   \n",
       "104                          https://data.gov.ua         CKAN   \n",
       "66        https://ckan.publishing.service.gov.uk         CKAN   \n",
       "211  https://scmb-ckan-dev.research.dc.uq.edu.au         CKAN   \n",
       "\n",
       "     number_of_datasets  number_of_resources            timestamp  \n",
       "81             647468.0            3064339.0  2023-08-14 00:19:50  \n",
       "50             250615.0            1554272.0  2023-08-13 21:24:06  \n",
       "104             29243.0             225757.0  2023-08-14 00:34:06  \n",
       "66              56145.0             213544.0  2023-08-13 21:31:36  \n",
       "211              1343.0             182581.0  2023-08-14 01:15:32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of checked Socrata portals: 125\n",
      "Total number of datasets on Socrata portals: 186,603\n",
      "Average number of datasets on Socrata portals: 1,493\n",
      "Total number of supported datasets on Socrata portals: 64,097\n",
      "Average number of supported datasets on Socrata portals: 513\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>api_software</th>\n",
       "      <th>number_of_datasets</th>\n",
       "      <th>number_of_supported_datasets</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.datos.gov.co</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>31484.0</td>\n",
       "      <td>6786.0</td>\n",
       "      <td>2023-08-15 15:30:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://colombia-mintic.data.socrata.com</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>31484.0</td>\n",
       "      <td>6786.0</td>\n",
       "      <td>2023-08-15 14:20:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://opendata.utah.gov</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>8043.0</td>\n",
       "      <td>2154.0</td>\n",
       "      <td>2023-08-15 14:52:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://data.ny.gov</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>5681.0</td>\n",
       "      <td>4172.0</td>\n",
       "      <td>2023-08-15 14:32:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://dati.lombardia.it</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>5542.0</td>\n",
       "      <td>3376.0</td>\n",
       "      <td>2023-08-15 14:40:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          url api_software  \\\n",
       "125                  https://www.datos.gov.co      Socrata   \n",
       "11   https://colombia-mintic.data.socrata.com      Socrata   \n",
       "98                  https://opendata.utah.gov      Socrata   \n",
       "44                        https://data.ny.gov      Socrata   \n",
       "68                  https://dati.lombardia.it      Socrata   \n",
       "\n",
       "     number_of_datasets  number_of_supported_datasets            timestamp  \n",
       "125             31484.0                        6786.0  2023-08-15 15:30:04  \n",
       "11              31484.0                        6786.0  2023-08-15 14:20:42  \n",
       "98               8043.0                        2154.0  2023-08-15 14:52:11  \n",
       "44               5681.0                        4172.0  2023-08-15 14:32:54  \n",
       "68               5542.0                        3376.0  2023-08-15 14:40:40  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>api_software</th>\n",
       "      <th>number_of_datasets</th>\n",
       "      <th>number_of_supported_datasets</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.datos.gov.co</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>31484.0</td>\n",
       "      <td>6786.0</td>\n",
       "      <td>2023-08-15 15:30:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://colombia-mintic.data.socrata.com</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>31484.0</td>\n",
       "      <td>6786.0</td>\n",
       "      <td>2023-08-15 14:20:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://data.ny.gov</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>5681.0</td>\n",
       "      <td>4172.0</td>\n",
       "      <td>2023-08-15 14:32:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://dati.lombardia.it</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>5542.0</td>\n",
       "      <td>3376.0</td>\n",
       "      <td>2023-08-15 14:40:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://bronx.lehman.cuny.edu</td>\n",
       "      <td>Socrata</td>\n",
       "      <td>4319.0</td>\n",
       "      <td>3209.0</td>\n",
       "      <td>2023-08-15 13:49:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          url api_software  \\\n",
       "125                  https://www.datos.gov.co      Socrata   \n",
       "11   https://colombia-mintic.data.socrata.com      Socrata   \n",
       "44                        https://data.ny.gov      Socrata   \n",
       "68                  https://dati.lombardia.it      Socrata   \n",
       "3               https://bronx.lehman.cuny.edu      Socrata   \n",
       "\n",
       "     number_of_datasets  number_of_supported_datasets            timestamp  \n",
       "125             31484.0                        6786.0  2023-08-15 15:30:04  \n",
       "11              31484.0                        6786.0  2023-08-15 14:20:42  \n",
       "44               5681.0                        4172.0  2023-08-15 14:32:54  \n",
       "68               5542.0                        3376.0  2023-08-15 14:40:40  \n",
       "3                4319.0                        3209.0  2023-08-15 13:49:41  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opendatasoft_statistics = pd.read_csv(\"data/portal_statistics_opendatasoft.csv\").dropna(subset = \"number_of_datasets\")\n",
    "print(\"Total number of checked Opendatasoft portals:\", \"{:,}\".format(int(len(opendatasoft_statistics[\"url\"]))))\n",
    "print(\"Total number of datasets on Opendatasoft portals:\", \"{:,}\".format(int(sum(opendatasoft_statistics[\"number_of_datasets\"]))))\n",
    "print(\"Average number of datasets on Opendatasoft portals:\", \"{:,}\".format(round(mean(opendatasoft_statistics[\"number_of_datasets\"]))))\n",
    "print(\"Total number of supported datasets on Opendatasoft portals:\", \"{:,}\".format(int(sum(opendatasoft_statistics[\"number_of_supported_datasets\"]))))\n",
    "print(\"Average number of supported datasets on Opendatasoft portals:\", \"{:,}\".format(round(mean(opendatasoft_statistics[\"number_of_supported_datasets\"]))))\n",
    "display(opendatasoft_statistics.sort_values(by = \"number_of_datasets\", ascending = False).head())\n",
    "display(opendatasoft_statistics.sort_values(by = \"number_of_supported_datasets\", ascending = False).head())\n",
    "\n",
    "ckan_statistics = pd.read_csv(\"data/portal_statistics_ckan.csv\").dropna(subset = \"number_of_datasets\")\n",
    "print(\"Total number of checked CKAN portals:\", \"{:,}\".format(int(len(ckan_statistics[\"url\"]))))\n",
    "print(\"Total number of datasets on CKAN portals:\", \"{:,}\".format(int(sum(ckan_statistics[\"number_of_datasets\"]))))\n",
    "print(\"Average number of datasets on CKAN portals:\", \"{:,}\".format(round(mean(ckan_statistics[\"number_of_datasets\"]))))\n",
    "print(\"Total number of resources on CKAN portals:\", \"{:,}\".format(int(sum(ckan_statistics[\"number_of_resources\"]))))\n",
    "print(\"Average number of resources on CKAN portals:\", \"{:,}\".format(round(mean(ckan_statistics[\"number_of_resources\"]))))\n",
    "display(ckan_statistics.sort_values(by = \"number_of_datasets\", ascending = False).head())\n",
    "display(ckan_statistics.sort_values(by = \"number_of_resources\", ascending = False).head())\n",
    "\n",
    "socrata_statistics = pd.read_csv(\"data/portal_statistics_socrata.csv\").dropna(subset = \"number_of_datasets\")\n",
    "print(\"Total number of checked Socrata portals:\", \"{:,}\".format(int(len(socrata_statistics[\"url\"]))))\n",
    "print(\"Total number of datasets on Socrata portals:\", \"{:,}\".format(int(sum(socrata_statistics[\"number_of_datasets\"]))))\n",
    "print(\"Average number of datasets on Socrata portals:\", \"{:,}\".format(round(mean(socrata_statistics[\"number_of_datasets\"]))))\n",
    "print(\"Total number of supported datasets on Socrata portals:\", \"{:,}\".format(int(sum(socrata_statistics[\"number_of_supported_datasets\"]))))\n",
    "print(\"Average number of supported datasets on Socrata portals:\", \"{:,}\".format(round(mean(socrata_statistics[\"number_of_supported_datasets\"]))))\n",
    "display(socrata_statistics.sort_values(by = \"number_of_datasets\", ascending = False).head())\n",
    "display(socrata_statistics.sort_values(by = \"number_of_supported_datasets\", ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
