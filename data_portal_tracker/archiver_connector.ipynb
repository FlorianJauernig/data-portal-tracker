{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import os\n",
    "import json\n",
    "import pymongo\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "\n",
    "class ArchiverConnector:\n",
    "    \"\"\"A class containing functions that connect to the Archiver API (using HTTP requests) and the Archiver database (via MongoDB queries).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode: str):\n",
    "        \"\"\"Instantiating the class.\n",
    "\n",
    "        Args:\n",
    "            mode (str): which MongoDB to connect to - must be \"local\" or \"production\"\n",
    "        \"\"\"      \n",
    "\n",
    "        # Loading environment variables \n",
    "        config = dotenv_values(\"../.env\")\n",
    "\n",
    "        # Setting Archiver API variables\n",
    "        self.archiver_base_url = config[\"ARCHIVER_BASE_URL\"]\n",
    "        self.archiver_password = config[\"ARCHIVER_PASSWORD\"]\n",
    "\n",
    "        # Connecting to the MongoDB\n",
    "        print(\"Connecting to MongoDB...\")\n",
    "\n",
    "        def check_mongodb(mongodb_url, mongodb_name):\n",
    "            self.archiver_database = pymongo.MongoClient(mongodb_url)[mongodb_name]\n",
    "            self.mapping_collection = self.archiver_database[\"datasets.mappings\"]\n",
    "            with pymongo.timeout(5):\n",
    "                self.mapping_collection.find_one({\"dataset_id\": \"12345\"})\n",
    "                print(\"Successfully connected.\")\n",
    "\n",
    "        if mode == \"local\":\n",
    "            try:\n",
    "                check_mongodb(config[\"LOCAL_MONGODB_URL\"], config[\"LOCAL_MONGODB_NAME\"])\n",
    "            except:\n",
    "                print(\"Connection failed.\")\n",
    "        elif mode == \"production\":\n",
    "            try:\n",
    "                check_mongodb(config[\"SERVER_MONGODB_URL_1\"], config[\"SERVER_MONGODB_NAME\"])\n",
    "            except:\n",
    "                print(\"Node 1 failed. Trying node 2...\")\n",
    "                try:\n",
    "                    check_mongodb(config[\"SERVER_MONGODB_URL_2\"], config[\"SERVER_MONGODB_NAME\"])\n",
    "                except:\n",
    "                    print(\"Node 2 failed. Trying node 3...\")\n",
    "                    try:\n",
    "                        check_mongodb(config[\"SERVER_MONGODB_URL_3\"], config[\"SERVER_MONGODB_NAME\"])\n",
    "                    except:\n",
    "                        print(\"Connection failed.\")\n",
    "\n",
    "    def api_get_dataset(self, dataset_url: str) -> dict:\n",
    "        \"\"\"Performing an API request to check if a dataset is indexed by the Archiver.\n",
    "\n",
    "        Args:\n",
    "            dataset_url (str): the URL of the dataset\n",
    "\n",
    "        Returns:\n",
    "            dict: {\"request_success\" = whether the request was successful, \\n\n",
    "                \"dataset_found\" = whether the dataset was found,  \\n\n",
    "                \"dataset_id\" = the ID of the dataset in the Archiver or None,  \\n\n",
    "                \"message\" = success message or failure message with details about the error}\n",
    "        \"\"\"\n",
    "\n",
    "        # Encoding the dataset URL\n",
    "        dataset_url = quote(dataset_url, safe = \"\")\n",
    "\n",
    "        # Building the request URL and defining the HTTP headers\n",
    "        archiver_request_url = self.archiver_base_url + 'api/v1/get/dataset/' + dataset_url\n",
    "\n",
    "        headers = {\n",
    "            'accept': 'application/json',\n",
    "        }\n",
    "\n",
    "        # Making the API call\n",
    "        try:\n",
    "            # Getting the response\n",
    "            response = requests.get(url = archiver_request_url, headers = headers)\n",
    "\n",
    "            # The request succeeded\n",
    "            if(response.status_code == 200):\n",
    "                # The dataset was found\n",
    "                try:\n",
    "                    return {\"request_success\": True, \"dataset_found\": True, \"dataset_id\": json.loads(response.content)[\"_id\"], \"message\": \"Success!\"}\n",
    "                # The dataset was not found (an exception occurred when accessing the \"_id\" value)\n",
    "                except Exception as exception:\n",
    "                    return {\"request_success\": True, \"dataset_found\": False, \"message\": \"Exception: \" + str(exception)}\n",
    "            # The request failed (the response code is not 200)\n",
    "            else:\n",
    "                return {\"request_success\": False, \"dataset_found\": False, \"message\": \"Response code: \" + str(response.status_code)}\n",
    "        # The request failed (an exception occurred when making the request)\n",
    "        except Exception as exception:\n",
    "            return {\"request_success\": False, \"dataset_found\": False, \"message\": \"Exception: \" + str(exception)}\n",
    "        \n",
    "    def api_add_dataset(self, dataset_url: str, source_url: str) -> dict:\n",
    "        \"\"\"Performing an API request to add a dataset to the Archiver.\n",
    "\n",
    "        Args:\n",
    "            dataset_url (str): the URL of the dataset\n",
    "            source_url (str): the URL of the dataset's source\n",
    "\n",
    "        Returns:\n",
    "            dict: {\"request_success\" = whether the request was successful, \\n\n",
    "                \"dataset_inserted\" = whether the dataset was inserted}, \\n\n",
    "                \"message\" = success message or failure message with details about the error}\n",
    "        \"\"\"\n",
    "\n",
    "        # Building the request URL and defining the HTTP headers, parameters and data\n",
    "        archiver_request_url = self.archiver_base_url + 'api/v1/post/resource'\n",
    "\n",
    "        headers = {\n",
    "            'accept': '*/*',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        params = (\n",
    "            ('secret', self.archiver_password),\n",
    "        )\n",
    "\n",
    "        data = json.dumps([{\"href\": str(dataset_url), \"source\": str(source_url)}])\n",
    "\n",
    "        # Making the API call\n",
    "        try:\n",
    "            # Getting the response\n",
    "            response = requests.post(url = archiver_request_url, headers = headers, params = params, data = data)\n",
    "            inserted = json.loads(response.content)[0][\"insertedDatasets\"]\n",
    "\n",
    "            # The request succeeded\n",
    "            if response.status_code == 200:\n",
    "                # The dataset was inserted\n",
    "                if inserted == 1:\n",
    "                    return {\"request_success\": True, \"dataset_inserted\": True, \"message\": \"Success!\"}\n",
    "                # The dataset was not inserted\n",
    "                elif inserted == 0:\n",
    "                    return {\"request_success\": True, \"dataset_inserted\": False, \"message\": \"Dataset not inserted!\"}\n",
    "            # The request failed (the response code is not 200)\n",
    "            else:\n",
    "                return {\"request_success\": False, \"dataset_inserted\": False, \"message\": \"Response code: \" + str(response.status_code)}\n",
    "        # The request failed (an exception occurred when making the request)\n",
    "        except Exception as exception:\n",
    "            return {\"request_success\": False, \"dataset_inserted\": False, \"message\": \"Exception: \" + str(exception)}\n",
    "    \n",
    "    def mongodb_get_mapping(self, dataset_id: str, metadata_id: str) -> dict:\n",
    "        \"\"\"Checking if there is an existing mapping between a dataset and its metadata in the \"datasets.mappings\" collection.\n",
    "\n",
    "        Args:\n",
    "            dataset_id (str): the ID of the dataset in the Archiver\n",
    "            metadata_id (str): the ID of the metadata in the Archiver\n",
    "\n",
    "        Returns:\n",
    "            dict: {\"query_success\" = whether the query was successful, \\n\n",
    "                \"dataset_found\" = whether the dataset was found in any mapping, \\n\n",
    "                \"metadata_found\" = whether the metadata was found in any mapping, \\n\n",
    "                \"mapping_found\" = whether a mapping between the dataset and the metadata was found, \\n\n",
    "                \"message\" = success message or failure message with details about the error}\n",
    "        \"\"\"\n",
    "\n",
    "        # Executing queries to determine if the dataset and metadata exist in the collection and if they are mapped to each other\n",
    "        try:\n",
    "            if self.mapping_collection.find_one({\"dataset_id\": dataset_id}) is None:\n",
    "                dataset_found = False\n",
    "            else:\n",
    "                dataset_found = True\n",
    "\n",
    "            if self.mapping_collection.find_one({\"metadata_id\": metadata_id}) is None:\n",
    "                metadata_found = False\n",
    "            else:\n",
    "                metadata_found = True\n",
    "\n",
    "            if self.mapping_collection.find_one({\"dataset_id\": dataset_id, \"metadata_id\": metadata_id}) is None:\n",
    "                mapping_found = False\n",
    "            else:\n",
    "                mapping_found = True\n",
    "\n",
    "            return {\"query_success\": True, \"dataset_found\": dataset_found, \"metadata_found\": metadata_found, \"mapping_found\": mapping_found, \"message\": \"Success!\"}\n",
    "        # The query failed, returning error details\n",
    "        except Exception as exception:\n",
    "            return {\"query_success\": False, \"message\": \"Exception: \" + str(exception)}\n",
    "\n",
    "    def mongodb_add_mapping(self, dataset_id: str, metadata_id: str) -> dict:\n",
    "        \"\"\"Adding a mapping entry for the dataset and metadata in the Archiver MongoDB.\n",
    "\n",
    "        Args:\n",
    "            dataset_id (str): the ID of the dataset in the Archiver\n",
    "            metadata_id (str): the ID of the metadata in the Archiver\n",
    "\n",
    "        Returns:\n",
    "            dict: {\"inserted\" = whether the mapping was inserted, \\n\n",
    "                \"mapping_id\" = the ID of the mapping document or None, \\n\n",
    "                \"message\" = success message or failure message with details about the error}\n",
    "        \"\"\"\n",
    "\n",
    "        # Creating the dictionary to be inserted as a document\n",
    "        document = {\"dataset_id\": dataset_id, \"metadata_id\": metadata_id, \"added\": datetime.now().isoformat()}\n",
    "\n",
    "        # Inserting the mapping document into the collection\n",
    "        try:\n",
    "            insert_status = self.mapping_collection.insert_one(document)\n",
    "            return {\"inserted\": insert_status.acknowledged, \"mapping_id\": insert_status.inserted_id, \"message\": \"Success!\"}\n",
    "        # The insertion failed, returning error details\n",
    "        except Exception as exception:\n",
    "            return {\"inserted\": False, \"message\": \"Exception: \" + str(exception)}\n",
    "        \n",
    "    def handle_dataset(self, dataset_url: str, metadata_url: str, source_url: str, log_file_success: str, log_file_fail: str) -> dict:\n",
    "        \"\"\"Checking if a dataset and its metadata are both indexed by the Archiver and have a mapping that describes their relation. Any missing indexing or mapping is added.\n",
    "\n",
    "        Args:\n",
    "            dataset_url (str): the URL of the dataset\n",
    "            metadata_url (str): the URL of the dataset's metadata\n",
    "            source_url (str): the URL of the dataset's source\n",
    "            log_file_success (str): the path of a CSV file logging successfully handled datasets\n",
    "            log_file_fail (str): the path of a CSV file logging datasets for which an exception occurred\n",
    "\n",
    "        Returns:\n",
    "            dict: {\"success\" = whether the process was successfully completed, \\n\n",
    "                \"dataset_added\" = whether the dataset was inserted via the API, \\n\n",
    "                \"metadata_added\" = whether the metadata was inserted via the API, \\n\n",
    "                \"mapping_added\" = whether a mapping between the dataset and the metadata was added via the MongoDB, \\n\n",
    "                \"dataset_id\" = the ID of the dataset in the Archiver or None, \\n\n",
    "                \"metadata_id\" = the ID of the metadata in the Archiver or None, \\n\n",
    "                \"message\" = success message or failure message with details about the error}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Getting the current timestamp\n",
    "        current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Waiting - adjust this in case of problems with excessive requests to the Archiver API!\n",
    "        sleep(1)\n",
    "\n",
    "        # Creating a dataframe in case the dataset can be handled successfully\n",
    "        completed_dataset = pd.DataFrame([(None, None, None, None, None, None, None, None, None, None)], columns = [\"timestamp\", \"dataset_url\", \"metadata_url\", \"source_url\", \"dataset_id\", \"metadata_id\", \"dataset_added\", \"metadata_added\", \"mapping_added\", \"message\"])\n",
    "        completed_datasets_filename = log_file_success\n",
    "\n",
    "        # Creating a dataframe in case the dataset cannot be handled successfully\n",
    "        failed_dataset = pd.DataFrame([(None, None, None, None, None, None, None, None)], columns = [\"timestamp\", \"dataset_url\", \"metadata_url\", \"source_url\", \"dataset_added\", \"metadata_added\", \"mapping_added\", \"message\"])\n",
    "        failed_datasets_filename = log_file_fail\n",
    "\n",
    "        # Initially setting the variables that log whether an error or certain relevant actions occurred to False\n",
    "        failed = False\n",
    "        dataset_added = False\n",
    "        metadata_added = False\n",
    "        mapping_added = False\n",
    "\n",
    "        # Creating a list containing the dataset URL and metadata URL\n",
    "        data_list = [{\"url\": dataset_url, \"type\": \"dataset\"}, \n",
    "                     {\"url\": metadata_url, \"type\": \"metadata\"}]\n",
    "\n",
    "        # For dataset and metadata, checking if the Archiver is indexing it already and adding it if necessary\n",
    "        for data in data_list:\n",
    "            # Checking if the dataset/metadata is indexed by the Archiver\n",
    "            check_data = self.api_get_dataset(data[\"url\"])\n",
    "\n",
    "            # The request to check the dataset/metadata failed, the loop stops\n",
    "            if check_data[\"request_success\"] == False:\n",
    "                failed = True\n",
    "                fail_reason = \"Initially getting the \" + data[\"type\"] + \" via the Archiver API failed.\"\n",
    "                break\n",
    "            # The dataset/metadata was not found\n",
    "            elif check_data[\"dataset_found\"] == False:\n",
    "                # Adding the dataset/metadata to the Archiver\n",
    "                add_data = self.api_add_dataset(data[\"url\"], source_url)\n",
    "\n",
    "                # The request to add the dataset/metadata failed or it wasn't inserted, the loop stops\n",
    "                if add_data[\"dataset_inserted\"] == False:\n",
    "                    failed = True\n",
    "                    fail_reason = \"Adding the \" + data[\"type\"] + \" via the Archiver API failed.\"\n",
    "                    break\n",
    "                # The dataset/metadata was inserted\n",
    "                elif add_data[\"dataset_inserted\"] == True:\n",
    "                    # Checking again if the dataset/metadata is indexed by the Archiver\n",
    "                    check_data = self.api_get_dataset(data[\"url\"])\n",
    "\n",
    "                    # The request to check the dataset/metadata failed or it wasn't found, the loop stops\n",
    "                    if check_data[\"dataset_found\"] == False:\n",
    "                        failed = True\n",
    "                        fail_reason = \"After successfully adding the \" + data[\"type\"] + \", getting it via the Archiver API failed.\"\n",
    "                        break\n",
    "                    # Logging the successful and verified insertion of the dataset/metadata\n",
    "                    elif check_data[\"dataset_found\"] == True:\n",
    "                        if data[\"type\"] == \"dataset\":\n",
    "                            dataset_added = True\n",
    "                        elif data[\"type\"] == \"metadata\":\n",
    "                            metadata_added = True\n",
    "\n",
    "            # The dataset/metadata was found (either directly or after adding it)\n",
    "            if check_data[\"dataset_found\"] == True:\n",
    "                # Saving the dataset ID\n",
    "                if data[\"type\"] == \"dataset\":\n",
    "                    dataset_id = check_data[\"dataset_id\"]\n",
    "                # Saving the metadata ID\n",
    "                elif data[\"type\"] == \"metadata\":\n",
    "                    metadata_id = check_data[\"dataset_id\"]\n",
    "\n",
    "        # No error occurred so far\n",
    "        if failed == False:\n",
    "            # Checking the dataset/metadata mapping\n",
    "            check_mapping = self.mongodb_get_mapping(dataset_id, metadata_id)\n",
    "\n",
    "            # The query to check the dataset/metadata mapping failed\n",
    "            if check_mapping[\"query_success\"] == False:\n",
    "                failed = True\n",
    "                fail_reason = \"Initially getting the dataset/metadata mapping via the Archiver MongoDB failed.\"\n",
    "            # The query succeeded, checking if the mapping exists already and adding it if necessary\n",
    "            elif check_mapping[\"query_success\"] == True:\n",
    "                # The mapping already exists\n",
    "                if check_mapping[\"mapping_found\"] == True:\n",
    "                    mapping_added = False\n",
    "                # The mapping doesn't exist yet, so it is now added\n",
    "                elif check_mapping[\"mapping_found\"] == False:\n",
    "                    add_mapping = self.mongodb_add_mapping(dataset_id, metadata_id)\n",
    "\n",
    "                    # The query to add the dataset/metadata mapping failed or it wasn't inserted\n",
    "                    if add_mapping[\"inserted\"] == False:\n",
    "                        failed = True\n",
    "                        fail_reason = \"Adding the mapping via the Archiver MongoDB failed.\"\n",
    "                    # Logging the successful insertion of the mapping\n",
    "                    elif add_mapping[\"inserted\"] == True:\n",
    "                            mapping_added = True\n",
    "\n",
    "                # Checking the dataset/metadata mapping again\n",
    "                check_mapping = self.mongodb_get_mapping(dataset_id, metadata_id)\n",
    "                if check_mapping[\"mapping_found\"] == False:\n",
    "                    failed = True\n",
    "                    fail_reason = \"After successfully adding the dataset/metadata mapping, getting it via the Archiver MongoDB failed.\"\n",
    "\n",
    "        # Saving the completed / failed dataset to the dataframe, then saving it to a new CSV file or appending it to an existing one\n",
    "        if failed == True:\n",
    "            failed_dataset.iloc[0,] = {\"timestamp\": current_timestamp, \"dataset_url\": dataset_url, \"metadata_url\": metadata_url, \"source_url\": source_url, \"dataset_added\": dataset_added, \"metadata_added\": metadata_added, \"mapping_added\": mapping_added, \"message\": \"Failed: \" + fail_reason}\n",
    "            failed_dataset.to_csv(failed_datasets_filename, mode = \"a\", index = False, header = not os.path.isfile(failed_datasets_filename))\n",
    "            return {\"success\": False, \"dataset_added\": dataset_added, \"metadata_added\": metadata_added, \"mapping_added\": mapping_added, \"message\": \"Failed: \" + fail_reason}\n",
    "        elif failed == False:\n",
    "            completed_dataset.iloc[0,] = {\"timestamp\": current_timestamp, \"dataset_url\": dataset_url, \"metadata_url\": metadata_url, \"source_url\": source_url, \"dataset_id\": dataset_id, \"metadata_id\": metadata_id, \"dataset_added\": dataset_added, \"metadata_added\": metadata_added, \"mapping_added\": mapping_added, \"message\": \"Success! Data and mapping complete!\"}\n",
    "            completed_dataset.to_csv(completed_datasets_filename, mode = \"a\", index = False, header = not os.path.isfile(completed_datasets_filename))\n",
    "            return {\"success\": True, \"dataset_added\": dataset_added, \"metadata_added\": metadata_added, \"mapping_added\": mapping_added, \"dataset_id\": dataset_id, \"metadata_id\": metadata_id, \"message\": \"Success! Data and mapping complete!\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing handle_dataset:\n",
    "archiver = ArchiverConnector(mode = \"production\")\n",
    "dataset_url = \"http://data.cookcountyil.gov/download/ikxe-tdm7\"\n",
    "metadata_url = \"http://data.cookcountyil.gov/api/views/metadata/v1/ikxe-tdm7\"\n",
    "source_url = \"http://data.cookcountyil.gov/d/ikxe-tdm7\"\n",
    "log_file_success = \"logs/handle_dataset_TEST_success.csv\"\n",
    "log_file_fail = \"logs/handle_dataset_TEST_fail.csv\"\n",
    "\n",
    "archiver.handle_dataset(\n",
    "    dataset_url = dataset_url,\n",
    "    metadata_url = metadata_url,\n",
    "    source_url = source_url,\n",
    "    log_file_success = log_file_success,\n",
    "    log_file_fail = log_file_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the \"crawl\" API method (not working!)\n",
    "archiver_request_url = 'https://archiver.ai.wu.ac.at/api/v1/crawl'\n",
    "\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "params = (\n",
    "    (\"id\", \"5e8571444c6fab0011dea345\"),\n",
    "    (\"hostname\", \"www.data.gv.at\"),\n",
    "    ('secret', 'dsArchiver'),\n",
    ")\n",
    "\n",
    "response = requests.post(url = archiver_request_url, headers = headers, params = params)\n",
    "for text in response:\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
