{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from helpers import check_protocol\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Extract search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_search_results(search_results_folder: str, output_file: str):\n",
    "    \"\"\"Extracting the URLs of organic search results from the saved JSON files\n",
    "\n",
    "    Code for looping through the search results was adapted from https://github.com/semantisch/crawley (Â© Daniil Dobriy)\n",
    "\n",
    "    Args:\n",
    "        ``search_results_folder (str):`` the path of the folder in the crawley-lite directory containing the search results \n",
    "\n",
    "        ``output_file (str):`` the path of the CSV file to be exported\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting the folder name\n",
    "    folder = search_results_folder\n",
    "\n",
    "    # Creating a dataframe for the portal URLs\n",
    "    search_results = pd.DataFrame(columns = [\"url\"])\n",
    "\n",
    "    # Looping through the result files\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        result_file = json.load(open(filepath, 'r', encoding='utf-8'))\n",
    "        # Considering only organic results\n",
    "        if \"organic_results\" in result_file:\n",
    "            # Add the organic results to the dataframe\n",
    "            for organic_result in result_file[\"organic_results\"]:\n",
    "                search_results.loc[len(search_results)] = organic_result[\"link\"]\n",
    "\n",
    "    # print(\"Number of result files:\", len(os.listdir(folder)))\n",
    "    # print(\"Number of organic search results:\", len(search_results))\n",
    "\n",
    "    # Exporting the list as a CSV file\n",
    "    search_results.to_csv(output_file, index = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a portal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list(search_results_file: str, output_file: str):\n",
    "    \"\"\"Creating an initial list of portal URLs based on multiple sources\n",
    "\n",
    "    Args:\n",
    "        ``search_results_file (str):`` the path of the CSV input file containing search result URLs in a column \"url\"\n",
    "\n",
    "        ``output_file (str):`` the path of the CSV file to be exported\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating a dataframe for the portal URLs\n",
    "    initial_portals = pd.DataFrame(columns = [\"url\"])\n",
    "\n",
    "    # Source 1 - Downloading portal lists and deserializing JSON\n",
    "    list_1_url = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/open-data-sources@public/exports/json\"\n",
    "    list_2_url = \"https://dataportals.org/api/data.json\"\n",
    "\n",
    "    list_1_response = requests.get(list_1_url)\n",
    "    list_2_response = requests.get(list_2_url)\n",
    "\n",
    "    list_1 = json.loads(list_1_response.content)\n",
    "    list_2 = json.loads(list_2_response.content)\n",
    "\n",
    "    # Source 2 - Manual additions to the list\n",
    "    additional_portals = [\n",
    "        {\"name\": \"OEBB\", \"url\": \"https://data.oebb.at\"},\n",
    "        {\"name\": \"DB\", \"url\": \"https://data.deutschebahn.com\"},\n",
    "        {\"name\": \"RENFE\", \"url\": \"https://data.renfe.com\"},\n",
    "        {\"name\": \"SNCF\", \"url\": \"https://data.sncf.com\"},\n",
    "        {\"name\": \"INFRABEL\", \"url\": \"https://opendata.infrabel.be\"},\n",
    "        {\"name\": \"SBB\", \"url\": \"https://data.sbb.ch\"},\n",
    "        {\"name\": \"PRORAIL\", \"url\": \"https://prorailnl.hub.arcgis.com\"},\n",
    "        {\"name\": \"Stanford\", \"url\": \"https://stanfordopendata.org\"},\n",
    "        {\"name\": \"Universidad de Alicante\", \"url\": \"https://transparencia.ua.es\"},\n",
    "        {\"name\": \"University of Southampton\", \"url\": \"https://data.southampton.ac.uk\"},\n",
    "        {\"name\": \"California State University\", \"url\": \"http://opendata.calstate.edu\"},\n",
    "        {\"name\": \"Department of Education\", \"url\": \"https://data.ed.gov\"},\n",
    "        {\"name\": \"University of Chicago\", \"url\": \"https://ucopendata.netlify.app\"},\n",
    "        {\"name\": \"University of Oxford\", \"url\": \"https://data.ox.ac.uk\"},\n",
    "        {\"name\": \"University of Edinburgh\", \"url\": \"https://datashare.ed.ac.uk\"},\n",
    "    ]\n",
    "\n",
    "    # Source 3 - Old portals from Open Data Portal Watch\n",
    "    portalwatch_portals = pd.read_csv(\"data/portalwatch_portals.csv\")\n",
    "    portalwatch_portals = portalwatch_portals[\"portal_url\"].values.tolist()\n",
    "\n",
    "    # Source 4 - Results from search engine queries\n",
    "    search_results = pd.read_csv(search_results_file)\n",
    "    search_results = search_results[\"url\"].values.tolist()\n",
    "\n",
    "    # Adding all portals to the dataframe\n",
    "    for portal in list_1:\n",
    "        initial_portals.loc[len(initial_portals)] = portal[\"url\"]\n",
    "\n",
    "    for portal in list_2:\n",
    "        initial_portals.loc[len(initial_portals)] = list_2[portal][\"url\"]\n",
    "\n",
    "    for portal in additional_portals:\n",
    "        initial_portals.loc[len(initial_portals)] = portal[\"url\"]\n",
    "\n",
    "    for portal in portalwatch_portals:\n",
    "        initial_portals.loc[len(initial_portals)] = portal\n",
    "\n",
    "    for portal in search_results:\n",
    "        initial_portals.loc[len(initial_portals)] = portal\n",
    "\n",
    "    # Basic first deduplication\n",
    "    initial_portals = initial_portals.drop_duplicates(ignore_index = True)\n",
    "\n",
    "    # Sorting the list\n",
    "    initial_portals = initial_portals.sort_values(\"url\", ignore_index = True)\n",
    "\n",
    "    # Exporting the list as a CSV file\n",
    "    initial_portals.to_csv(output_file, index = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deduplicate the portal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(initial_portals_file: str, output_file: str):\n",
    "    \"\"\"Removing duplicates from an input list of URLs\n",
    "\n",
    "    Args:\n",
    "        ``initial_portals_file (str):`` the path of the CSV input file containing initial portal URLs in a column \"url\"\n",
    "        \n",
    "        ``output_file (str):`` the path of the CSV file to be exported\n",
    "    \"\"\"\n",
    "\n",
    "    # Opening the file that contains the initial portal URLs\n",
    "    initial_portals = pd.read_csv(initial_portals_file)\n",
    "\n",
    "    # Looping through the portal URLs\n",
    "    for index, portal in initial_portals.iterrows():\n",
    "        # Removing slashes and number signs at the end of the URL\n",
    "        while (portal[\"url\"].endswith(\"/\") or portal[\"url\"].endswith(\"#\")):\n",
    "            initial_portals.loc[index, \"url\"] = portal[\"url\"].rstrip(\"/\").rstrip(\"#\")\n",
    "\n",
    "        # Removing whitespace and quotes around the URL\n",
    "        initial_portals.loc[index, \"url\"] = portal[\"url\"].strip().strip('\\\"').strip(\"\\'\")\n",
    "\n",
    "        # Shortening the URL to the base URL and removing the HTTP(S) protocol prefix\n",
    "        initial_portals.loc[index, \"url\"] = urlparse(portal[\"url\"]).netloc\n",
    "\n",
    "    # Saving the duplicates to a dataframe and exporting it as a CSV file\n",
    "    # duplicate_portals = initial_portals[initial_portals.duplicated()].sort_values(by=[\"url\"])\n",
    "    # duplicate_portals.to_csv(\"data/duplicate_portals.csv\", index = None)\n",
    "\n",
    "    # Saving the unique values to a dataframe and exporting it as a CSV file\n",
    "    deduplicated_portals = initial_portals.drop_duplicates(ignore_index = True).sort_values(by=[\"url\"])\n",
    "    deduplicated_portals.to_csv(output_file, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add manually validated API endpoints\n",
    "\n",
    "The function add_api_endpoints() addresses two issues:\n",
    "- Some portals have API endpoints that use a non-standard path (e.g. /catalog/api instead of /api).\n",
    "- The second validation step of the validate_list() function only checks the API functionality of portals for which HTML markers were found in the first validation step.\n",
    "\n",
    "Therefore, the function add_api_endpoints() can be used to add portals which use a custom path or are known to support a certain API, but do not have any HTML markers. If they are added in this way, the first validation step of the validate_list() function is bypassed and the portals will end up on the final portal list. Also note that portals added like this will be counted as \"suspected\" for their respective portal software in the validation statistics - thus, the number of \"suspected\" portals is not fully equivalent to the number of sites for which portal HTML markers were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_api_endpoints(manual_api_additions_file: str, deduplicated_portals_file: str, output_file: str):\n",
    "    \"\"\"Adding known portal API endpoints that are not reachable from the base URL (\"/api\") but via a different path (like \"/catalog/api\") or do not have any HTML markers\n",
    "\n",
    "    Args:\n",
    "        ``manual_api_additions_file (str):`` the path of the CSV input file containing API base URLs without \"/api/...\" in a column \"url\", e.g. \"data.gv.at/katalog\", and the API software name or \"Unknown\" in a column \"manually_checked_api\"\n",
    "        \n",
    "        ``deduplicated_portals_file (str):`` the path of the CSV input file containing deduplicated portal URLs in a column \"url\"\n",
    "\n",
    "        ``output_file (str):`` the path of the CSV file to be exported\n",
    "    \"\"\"\n",
    "\n",
    "    # Opening the file that contains the API endpoints\n",
    "    manual_api_additions = pd.read_csv(manual_api_additions_file)\n",
    "\n",
    "    # Opening the file that contains the unique / deduplicated portal URLs (without protocol prefixes)\n",
    "    deduplicated_portals = pd.read_csv(deduplicated_portals_file)\n",
    "\n",
    "    # Adding the API endpoints to the portal list\n",
    "    extended_portals = pd.concat([deduplicated_portals, manual_api_additions], ignore_index = True).drop_duplicates().sort_values(by = \"url\")\n",
    "\n",
    "    # Exporting the extended list to a CSV file\n",
    "    extended_portals.to_csv(output_file, index = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add protocol prefixes and activity status\n",
    "\n",
    "For each entry in the input list, the function requests the URL with HTTPS, then falls back to HTTP in case of an exception or a response code indicating failure. Information about the supported protocol and the website activity status is added to each row in the dataframe and the enriched list is exported to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefixes(extended_portals_file: str, output_file: str):\n",
    "    \"\"\"Iterating over a portal list, adding the protocol prefix (if it is working) and adding a portal activity status\n",
    "\n",
    "    Args:\n",
    "        ``extended_portals_file (str):`` the path of the CSV input file containing portal URLs in a column \"url\"\n",
    "        \n",
    "        ``output_file (str):`` the path of the CSV file to be exported\n",
    "    \"\"\"\n",
    "    \n",
    "    # Opening the file that contains the unique / deduplicated portal URLs (without protocol prefixes)\n",
    "    extended_portals = pd.read_csv(extended_portals_file)\n",
    "\n",
    "    # Adding a column that indicates whether or not the portal is active, i.e. responds to an HTTP request.\n",
    "    extended_portals[\"active\"] = pd.Series(dtype = \"boolean\")\n",
    "    \n",
    "    # Adding a column for error information\n",
    "    extended_portals[\"error_type\"] = None\n",
    "\n",
    "    # Iterating over all portals in the list\n",
    "    for index, portal in extended_portals.iterrows():\n",
    "        # Print current portal and its position in the list\n",
    "        print(\"Portal \" + str(index + 1) + \"/\" + str(len(extended_portals)) + \": \" + portal[\"url\"])\n",
    "\n",
    "        # Check protocol and update portal URL in the list\n",
    "        result = check_protocol(portal[\"url\"])\n",
    "        extended_portals.loc[index, \"url\"] = result\n",
    "\n",
    "        # HTTPS worked\n",
    "        if (result.startswith(\"https://\")):\n",
    "            print(\"Added to list as active portal with HTTPS. \\n\")\n",
    "            extended_portals.loc[index, \"active\"] = True\n",
    "        # HTTP worked\n",
    "        elif (result.startswith(\"http://\")):\n",
    "            print(\"Added to list as active portal with HTTP. \\n\")\n",
    "            extended_portals.loc[index, \"active\"] = True\n",
    "        # Neither worked\n",
    "        else:\n",
    "            print(\"Added to list as inactive portal. \\n\")\n",
    "            extended_portals.loc[index, \"active\"] = False\n",
    "            extended_portals.loc[index, \"error_type\"] = [\"HTTPS and HTTP requests failed\"]\n",
    "        \n",
    "    # Sorting the dataframe by URL and saving it to a CSV file\n",
    "    prefixed_portals = extended_portals.sort_values(by=[\"url\"])\n",
    "    prefixed_portals.to_csv(output_file, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_list(input_list: str, output_list: str, output_markers: str, input_markers: str = None, retry_failed_portals: bool = False):\n",
    "    \"\"\"Iterating over a portal list, validating that the portals use a relevant catalog software and exporting the validation results\n",
    "\n",
    "    Code for checking validation markers and the related JSON export was partially taken from https://github.com/semantisch/crawley (Â© Daniil Dobriy)\n",
    "    \n",
    "    Args:\n",
    "        ``input_list (str):`` the path of the CSV input file containing URLs - must be a file created previously by \"add_prefixes()\" or \"validate_list()\" - if \"retry_failed_portals\" is True, must be a file created previously by \"validate_list()\"\n",
    "        \n",
    "        ``output_list (str):`` the path of the CSV output file to be exported, containing validated URLs\n",
    "        \n",
    "        ``output_markers (str):`` the path of the JSON output file to be exported, containing portals and their detected validation markers\n",
    "        \n",
    "        ``input_markers (str, optional):`` the path of the JSON input file containing portals and their detected validation markers - must be a file created previously by \"validate_list()\"\n",
    "        \n",
    "        ``retry_failed_portals (bool, optional):`` whether or not to retry the portals for which the validation failed or the suspected API didn't work in a previous run - defaults to False\n",
    "    \"\"\"\n",
    "\n",
    "    # Opening the file that contains the portal URLs\n",
    "    prefixed_portals = pd.read_csv(input_list)\n",
    "\n",
    "    # Only the active portals will be validated\n",
    "    active_portals = prefixed_portals[prefixed_portals[\"active\"] == True]\n",
    "\n",
    "    # Retrying only those portals from a previous validation run for which the validation failed or the suspected API didn't work\n",
    "    if retry_failed_portals == True:\n",
    "        active_portals = active_portals[(active_portals[\"validated\"] == False) | (active_portals[\"api_working\"] == False)]\n",
    "\n",
    "    # Adding columns that indicate if the portal could be validated and which catalog software it uses\n",
    "    if retry_failed_portals == False:\n",
    "        prefixed_portals[\"validated\"] = False\n",
    "        prefixed_portals[\"suspected_api\"] = None\n",
    "        prefixed_portals[\"api_working\"] = None\n",
    "        prefixed_portals[\"api_version\"] = None\n",
    "\n",
    "    # Defining an inner function to log errors\n",
    "    def log(error):\n",
    "        errors.append(str(type(error).__name__))\n",
    "        prefixed_portals.loc[index, \"error_type\"] = str(errors)\n",
    "\n",
    "    # Loading the validation markers\n",
    "    config = json.load(open('../crawley-lite/config.json', 'r', encoding='utf-8'))\n",
    "\n",
    "    # Creating or loading a dictionary for the portals with detected validation markers\n",
    "    if retry_failed_portals == False or input_markers is None:\n",
    "        validated_sites = {}\n",
    "    else:\n",
    "        with open(input_markers, \"r\", encoding = 'utf8') as file:\n",
    "            validated_sites = json.load(file)\n",
    "\n",
    "    # Showing information\n",
    "    if retry_failed_portals == False:\n",
    "        print(\"Skipping inactive portals...\")\n",
    "    else:\n",
    "        print(\"Skipping all portals except those that previously could not be validated or had non-working APIs...\")\n",
    "\n",
    "    # Iterating over all active portals in the list\n",
    "    for index, portal in active_portals.iterrows():\n",
    "        # Print current portal and its position in the list of all portals\n",
    "        print(\"Portal \" + str(index + 1) + \"/\" + str(len(prefixed_portals)) + \": \" + portal[\"url\"])\n",
    "\n",
    "        # Setting the portal URL as the base URL\n",
    "        base_url = portal[\"url\"]\n",
    "\n",
    "        # Setting the variable that is used to stop the loop when the software is found\n",
    "        software_found = False\n",
    "\n",
    "        # (Re-)setting the errors variable\n",
    "        errors = []\n",
    "\n",
    "        # Removing the previous error when retrying a portal\n",
    "        if retry_failed_portals == True:\n",
    "            prefixed_portals.loc[index, \"error_type\"] = None\n",
    "\n",
    "        # Requesting the site and retrieve its contents\n",
    "        try:\n",
    "            response = requests.get(base_url, timeout = 15)\n",
    "            contents = response.text\n",
    "        # If the request fails / times out, skipping to the next portal\n",
    "        except Exception as e:\n",
    "            log(e)\n",
    "            continue\n",
    "        \n",
    "        # Searching for markers of data catalog platforms (CKAN, etc.) if the API software is not yet known\n",
    "        if pd.isna(prefixed_portals.loc[index, \"manually_checked_api\"]):\n",
    "            for platform_type in config:\n",
    "                # Looping through validation markers for each platform\n",
    "                for validation_marker in config[platform_type][\"validate\"]:\n",
    "                    # Adding platform type to the dictionary if not there yet\n",
    "                    if not platform_type in validated_sites:\n",
    "                        validated_sites[platform_type] = {}\n",
    "                    # Checking if the validation marker can be found in the current site's contents\n",
    "                    if validation_marker.lower() in contents.lower():\n",
    "                        # Adding the site to the validated sites for the platform\n",
    "                        if not base_url in validated_sites[platform_type]:\n",
    "                            validated_sites[platform_type][base_url] = []\n",
    "                        # Adding the validation marker that was found\n",
    "                        if validation_marker not in validated_sites[platform_type][base_url]:\n",
    "                            validated_sites[platform_type][base_url].append(validation_marker)\n",
    "                        # Setting variable to stop looping through the platforms\n",
    "                        software_found = True\n",
    "                # If a validation marker was found, saving the platform software\n",
    "                if base_url in validated_sites[platform_type]:\n",
    "                    prefixed_portals.loc[index, \"suspected_api\"] = platform_type\n",
    "                    print(platform_type, \"markers found\")\n",
    "                # If no validation marker was found, marking the software as unknown\n",
    "                else:\n",
    "                    prefixed_portals.loc[index, \"suspected_api\"] = \"Unknown\"\n",
    "\n",
    "                # Setting the portal as validated\n",
    "                prefixed_portals.loc[index, \"validated\"] = True\n",
    "                \n",
    "                # Stopping the loop through the platforms\n",
    "                if software_found == True:\n",
    "                    break\n",
    "        # Not searching for markers if the API software is already known (was manually checked before)\n",
    "        else:\n",
    "            # Save the manually checked API as the suspected API software\n",
    "            prefixed_portals.loc[index, \"suspected_api\"] = portal[\"manually_checked_api\"]\n",
    "            print(portal[\"manually_checked_api\"], \"portal was manually added, skipping search for markers\")\n",
    "\n",
    "            # Setting the portal as validated\n",
    "            prefixed_portals.loc[index, \"validated\"] = True\n",
    "\n",
    "        # Verifying that the detected API is available and working\n",
    "        if prefixed_portals.loc[index, \"suspected_api\"] is not None and prefixed_portals.loc[index, \"suspected_api\"] != \"Unknown\":\n",
    "\n",
    "            # Checking portals with CKAN markers\n",
    "            if prefixed_portals.loc[index, \"suspected_api\"] == \"CKAN\":\n",
    "                # Resetting version variable\n",
    "                ckan_version = None\n",
    "                \n",
    "                try:\n",
    "                    api_url = base_url + \"/api/3/action/package_search\"\n",
    "                    response = requests.get(api_url, timeout = 15)\n",
    "                    if json.loads(response.text)[\"success\"] == True:\n",
    "                        print(\"CKAN API working\")\n",
    "                        prefixed_portals.loc[index, \"api_working\"] = True\n",
    "                        # Checking the API version\n",
    "                        try:\n",
    "                            api_version_url = base_url + \"/api/3/action/status_show\" \n",
    "                            response = requests.get(api_version_url, timeout = 15)\n",
    "                            ckan_version = json.loads(response.text)[\"result\"][\"ckan_version\"]\n",
    "                            prefixed_portals.loc[index, \"api_version\"] = ckan_version\n",
    "                        except Exception as e:\n",
    "                            prefixed_portals.loc[index, \"api_version\"] = \"Unknown\"\n",
    "                            log(e)\n",
    "                except Exception as e:\n",
    "                    print(\"CKAN API not working\")\n",
    "                    prefixed_portals.loc[index, \"api_working\"] = False\n",
    "                    log(e)\n",
    "\n",
    "            # Checking portals with Socrata markers\n",
    "            elif prefixed_portals.loc[index, \"suspected_api\"] == \"Socrata\":\n",
    "                try:\n",
    "                    api_url = base_url + \"/api/views/metadata/v1?method=help\"\n",
    "                    response = requests.get(api_url, timeout = 15)\n",
    "                    if \"id\" in json.loads(response.text)[\"immutableFields\"]:\n",
    "                        print(\"Socrata API working\")\n",
    "                        prefixed_portals.loc[index, \"api_working\"] = True\n",
    "                        # The \"Socrata Metadata API\" (not \"SODA API\"!) seems to only have one version\n",
    "                        prefixed_portals.loc[index, \"api_version\"] = \"v1.0\"\n",
    "                except Exception as e:\n",
    "                    print(\"Socrata API not working\")\n",
    "                    prefixed_portals.loc[index, \"api_working\"] = False\n",
    "                    log(e)\n",
    "\n",
    "            # Checking portals with Opendatasoft markers\n",
    "            elif prefixed_portals.loc[index, \"suspected_api\"] == \"OpenDataSoft\":\n",
    "                # Reset versions variable\n",
    "                opendatasoft_versions = None\n",
    "\n",
    "                # Check API v2.x\n",
    "                try:\n",
    "                    api_url = base_url + \"/api/explore/\"\n",
    "                    response = requests.get(api_url, timeout = 15)\n",
    "                    opendatasoft_versions = json.loads(response.text)[\"versions\"]\n",
    "                    print(\"Opendatasoft API v2.x working\")\n",
    "                except Exception as e:\n",
    "                    print(\"Opendatasoft API v2.x not working\")\n",
    "                    log(e)\n",
    "\n",
    "                # Check API v1.0\n",
    "                try:\n",
    "                    api_url_old = base_url + \"/api/datasets/1.0/search/?rows=1\"\n",
    "                    response = requests.get(api_url_old, timeout = 15)\n",
    "                    # Trying to access a JSON key of a valid API response  \n",
    "                    json.loads(response.text)[\"nhits\"]\n",
    "                    try:\n",
    "                        opendatasoft_versions.insert(0, \"v1.0\")\n",
    "                    except NameError:\n",
    "                        opendatasoft_versions = \"v1.0\"\n",
    "                    print(\"Opendatasoft API v1.0 working\")\n",
    "                except Exception as e:\n",
    "                    print(\"Opendatasoft API v1.0 not working\")\n",
    "                    log(e)\n",
    "\n",
    "                # Saving the working API versions\n",
    "                if opendatasoft_versions is not None:\n",
    "                    prefixed_portals.loc[index, \"api_version\"] = str(opendatasoft_versions)\n",
    "                    prefixed_portals.loc[index, \"api_working\"] = True\n",
    "                else:\n",
    "                    prefixed_portals.loc[index, \"api_working\"] = False\n",
    "\n",
    "    # Showing information\n",
    "    if retry_failed_portals == False:\n",
    "        print(\"Skipping inactive portals...\")\n",
    "    else:\n",
    "        print(\"Skipping all portals except those that previously could not be validated or had non-working APIs...\")\n",
    "\n",
    "    # Exporting the validation results to a JSON file\n",
    "    with open(output_markers, \"w\", encoding = 'utf8') as outfile:\n",
    "        json.dump(validated_sites, outfile, ensure_ascii = False, indent = 4)\n",
    "\n",
    "    # Printing the number of validated sites per platform type\n",
    "    for platform_type in config:\n",
    "        if platform_type in validated_sites:\n",
    "            detected_markers = len(validated_sites[platform_type])\n",
    "        else:\n",
    "            detected_markers = 0\n",
    "        print(f'\\nPortals with detected {platform_type} markers: {detected_markers}')\n",
    "        print(f'Portals with a manually added {platform_type} API endpoint: {len(prefixed_portals[prefixed_portals[\"manually_checked_api\"] == platform_type])}')\n",
    "        print(f'Portals with a working {platform_type} API: {len(prefixed_portals[(prefixed_portals[\"suspected_api\"] == platform_type) & (prefixed_portals[\"api_working\"] == True)])}')\n",
    "\n",
    "    # Ordering the dataframe, sorting it by URL and saving it to a CSV file\n",
    "    prefixed_portals = prefixed_portals[[\"url\", \"active\", \"validated\", \"manually_checked_api\", \"suspected_api\", \"api_working\", \"api_version\", \"error_type\"]]\n",
    "    prefixed_portals = prefixed_portals.sort_values(by=[\"url\"])\n",
    "    prefixed_portals.to_csv(output_list, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze the validated list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_list(validated_portals_file: str, show: str = True, export: bool = False):\n",
    "    \"\"\"Analyzing, presenting and saving the most important information about a validated portal list\n",
    "\n",
    "    Args:\n",
    "        ``validated_portals_file (str):`` the path of the CSV input file containing validated portal URLs - must be a file created previously by \"validate_list()\"\n",
    "\n",
    "        ``show (bool, optional):`` whether or not to display the relevant dataframes and results - defaults to True\n",
    "\n",
    "        ``export (bool, optional):`` whether or not to append the results to the statistics CSV file - defaults to False\n",
    "    \"\"\"    \n",
    "\n",
    "    # Importing the list of validated portals \n",
    "    file = validated_portals_file\n",
    "    validated_portals = pd.read_csv(file)\n",
    "\n",
    "    # Creating a dataframe for the statistics\n",
    "    statistics = pd.DataFrame(columns = [\"file\", \"total\", \"active\", \"inactive\", \"validated\", \"unvalidated\", \"subpage_endpoints\", \"no_markers\", \"ckan_suspected\", \"ckan_working\", \"opendatasoft_suspected\", \"opendatasoft_working\", \"socrata_suspected\", \"socrata_working\", \"timestamp\"])\n",
    "\n",
    "    # File path, total portals, file modification timestamp\n",
    "    statistics.loc[0, \"file\"] = file\n",
    "    statistics.loc[0, \"total\"] = len(validated_portals)\n",
    "    file_modification_unix_time = os.path.getmtime(file)\n",
    "    timestamp = str(datetime.datetime.fromtimestamp(file_modification_unix_time))\n",
    "    statistics.loc[0, \"timestamp\"] = timestamp\n",
    "\n",
    "    # Showing the unique values of each column\n",
    "    if show is True:\n",
    "        for column in validated_portals:\n",
    "            print(str(column) + \": \" + str(validated_portals[column].unique()))\n",
    "\n",
    "    # Portals that are active\n",
    "    active_portals = validated_portals[validated_portals[\"active\"] == True]\n",
    "    active_portals.columns.name = 'Active portals'\n",
    "    statistics.loc[0, \"active\"] = len(active_portals)\n",
    "    if show is True:\n",
    "        display(active_portals)\n",
    "\n",
    "    # Portals that are inactive (i.e. did not respond)\n",
    "    inactive_portals = validated_portals[validated_portals[\"active\"] == False].copy()\n",
    "    inactive_portals.columns.name = 'Inactive portals'\n",
    "    statistics.loc[0, \"inactive\"] = len(inactive_portals)\n",
    "    if show is True:\n",
    "        display(inactive_portals)\n",
    "\n",
    "    # Portals that could be validated\n",
    "    successfully_validated_portals = validated_portals[validated_portals[\"validated\"] == True]\n",
    "    successfully_validated_portals.columns.name = \"Validated portals\"\n",
    "    statistics.loc[0, \"validated\"] = len(successfully_validated_portals)\n",
    "    if show is True:\n",
    "        display(successfully_validated_portals)\n",
    "\n",
    "    # Portals that could not be validated (e.g. due to a timeout)\n",
    "    unvalidated_portals = validated_portals[validated_portals[\"validated\"] == False]\n",
    "    unvalidated_portals.columns.name = \"Failed / unvalidated portals\"\n",
    "    statistics.loc[0, \"unvalidated\"] = len(unvalidated_portals)\n",
    "    if show is True:\n",
    "        display(unvalidated_portals)\n",
    "\n",
    "    # Portals with non-standard subpage API endpoints that were manually checked and added\n",
    "    subpage_endpoint_portals = validated_portals[validated_portals[\"manually_checked_api\"].notna()]\n",
    "    subpage_endpoint_portals.columns.name = \"Portals with non-standard subpage API endpoints\"\n",
    "    statistics.loc[0, \"subpage_endpoints\"] = len(subpage_endpoint_portals)\n",
    "    if show is True:\n",
    "        display(subpage_endpoint_portals)\n",
    "\n",
    "    # Portals for which no validation markers were found\n",
    "    no_markers_portals = validated_portals[validated_portals[\"suspected_api\"] == \"Unknown\"]\n",
    "    no_markers_portals.columns.name = \"Portals without markers\"\n",
    "    statistics.loc[0, \"no_markers\"] = len(no_markers_portals)\n",
    "    if show is True:\n",
    "        display(no_markers_portals)\n",
    "\n",
    "    # Suspected CKAN portals (validation markers found or API manually checked) \n",
    "    ckan_markers_portals = validated_portals[validated_portals[\"suspected_api\"] == \"CKAN\"]\n",
    "    ckan_markers_portals.columns.name = \"Suspected CKAN portals\"\n",
    "    statistics.loc[0, \"ckan_suspected\"] = len(ckan_markers_portals)\n",
    "    if show is True:\n",
    "        display(ckan_markers_portals)\n",
    "\n",
    "    # Portals with working CKAN API\n",
    "    ckan_working_api_portals = ckan_markers_portals[ckan_markers_portals[\"api_working\"] == True]\n",
    "    ckan_working_api_portals.columns.name = \"Portals with working CKAN API\"\n",
    "    statistics.loc[0, \"ckan_working\"] = len(ckan_working_api_portals)\n",
    "    if show is True:\n",
    "        display(ckan_working_api_portals)\n",
    "\n",
    "    # Suspected Opendatasoft portals (validation markers found or API manually checked) \n",
    "    opendatasoft_markers_portals = validated_portals[validated_portals[\"suspected_api\"] == \"OpenDataSoft\"]\n",
    "    opendatasoft_markers_portals.columns.name = \"Suspected Opendatasoft portals\"\n",
    "    statistics.loc[0, \"opendatasoft_suspected\"] = len(opendatasoft_markers_portals)\n",
    "    if show is True:\n",
    "        display(opendatasoft_markers_portals)\n",
    "\n",
    "    # Portals with working Opendatasoft API\n",
    "    opendatasoft_working_api_portals = opendatasoft_markers_portals[opendatasoft_markers_portals[\"api_working\"] == True]\n",
    "    opendatasoft_working_api_portals.columns.name = \"Portals with working Opendatasoft API\"\n",
    "    statistics.loc[0, \"opendatasoft_working\"] = len(opendatasoft_working_api_portals)\n",
    "    if show is True:\n",
    "        display(opendatasoft_working_api_portals)\n",
    "\n",
    "    # Suspected Socrata portals (validation markers found or API manually checked) \n",
    "    socrata_markers_portals = validated_portals[validated_portals[\"suspected_api\"] == \"Socrata\"]\n",
    "    socrata_markers_portals.columns.name = \"Suspected Socrata portals\"\n",
    "    statistics.loc[0, \"socrata_suspected\"] = len(socrata_markers_portals)\n",
    "    if show is True:\n",
    "        display(socrata_markers_portals)\n",
    "\n",
    "    # Portals with working Socrata API\n",
    "    socrata_working_api_portals = socrata_markers_portals[socrata_markers_portals[\"api_working\"] == True]\n",
    "    socrata_working_api_portals.columns.name = \"Portals with working Socrata API\"\n",
    "    statistics.loc[0, \"socrata_working\"] = len(socrata_working_api_portals)\n",
    "    if show is True:\n",
    "        display(socrata_working_api_portals)\n",
    "\n",
    "    # Showing statistics\n",
    "    if show is True:\n",
    "        display(statistics)\n",
    "\n",
    "    # If the CSV file exists, checking if the portal list statistics are included in it already (same file path and modification timestamp)\n",
    "    if export is True:\n",
    "        try:\n",
    "            existing_csv = pd.read_csv(\"data/validation_statistics.csv\")\n",
    "            if not (file in str(existing_csv[\"file\"]) and timestamp in str(existing_csv[\"timestamp\"])):\n",
    "                raise Exception\n",
    "            else:\n",
    "                print(\"Statistics for this list are already in data/validation_statistics.csv\")\n",
    "        # If the CSV file doesn't exist or the portal list statistics are not included yet, create a new CSV or append to the existing one\n",
    "        except:\n",
    "            statistics.to_csv(\"data/validation_statistics.csv\", mode = \"a\", index = False, header = not os.path.isfile(\"data/validation_statistics.csv\"))\n",
    "            print(\"Statistics were saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract portals with working APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_working_apis(validated_portals_file: str, output_file: str):\n",
    "    \"\"\"Extracting the essential data from the validated portal list, keeping only the portals with working APIs, performing a final deduplication and exporting the final list\n",
    "\n",
    "    Args:\n",
    "        ``validated_portals_file (str):`` the path of the CSV input file containing validated portal URLs - must be a file created previously by \"validate_list()\"\n",
    "        \n",
    "        ``output_file (str):`` the path of the CSV file to be exported, containing the final list of portal APIs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Opening the file that contains the validated portal URLs\n",
    "    validated_portals = pd.read_csv(validated_portals_file)\n",
    "\n",
    "    # Keeping only the portals with a working API\n",
    "    working_portals = validated_portals[validated_portals[\"api_working\"] == True].copy()\n",
    "\n",
    "    # Removing \"www.\" from the netloc and saving in a new column to find duplicates that appear with and without \"www.\"\n",
    "    for index, portal in working_portals.iterrows():\n",
    "        working_portals.loc[index, \"netloc_without_www\"] = urlparse(portal[\"url\"]).netloc.removeprefix(\"www.\")\n",
    "\n",
    "    # Finding all duplicate URLs and keeping both the first occurences and the duplicates\n",
    "    # display(working_portals[working_portals.duplicated(\"netloc_without_www\", False)].sort_values([\"netloc_without_www\", \"url\"]))\n",
    "\n",
    "    # Counting the number of duplicates (without the first occurences)\n",
    "    # print(\"Duplicate sites after removing \\\"www.\\\":\", len(working_portals[working_portals.duplicated(\"netloc_without_www\")]))\n",
    "\n",
    "    # Removing the duplicates\n",
    "    final_portals = working_portals.drop_duplicates(subset = \"netloc_without_www\", keep = \"first\", ignore_index = True) \n",
    "\n",
    "    # Showing the final portal list\n",
    "    # display(final_portals)\n",
    "\n",
    "    # Editing the columns, sorting the dataframe by URL and saving it to a CSV file\n",
    "    final_portals = final_portals.rename(columns = {\"suspected_api\": \"api_software\"})\n",
    "    final_portals = final_portals[[\"url\", \"api_working\", \"api_software\", \"api_version\"]].sort_values(by=[\"url\"])\n",
    "    final_portals.to_csv(output_file, index = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Call the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>api_working</th>\n",
       "      <th>api_software</th>\n",
       "      <th>api_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://101.79.9.128</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://103.231.194.8</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://116.203.208.239</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://129.194.213.24</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://130.179.67.140</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://185.75.55.180</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://192.241.154.193</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://3.237.95.96</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://84.38.48.220</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://ambar.utpl.edu.ec</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>https://www.offenesdatenportal.de</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>https://www.opendata-paysbasque.fr</td>\n",
       "      <td>True</td>\n",
       "      <td>OpenDataSoft</td>\n",
       "      <td>['v1.0', 'v2.0', 'v2.1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>https://www.opendata.corsica</td>\n",
       "      <td>True</td>\n",
       "      <td>OpenDataSoft</td>\n",
       "      <td>['v1.0', 'v2.0', 'v2.1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>https://www.opendata.nhs.scot</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>https://www.opendata56.fr</td>\n",
       "      <td>True</td>\n",
       "      <td>OpenDataSoft</td>\n",
       "      <td>['v1.0', 'v2.0', 'v2.1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>https://www.opentourism.net</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>https://www.projets-environnement.gouv.fr</td>\n",
       "      <td>True</td>\n",
       "      <td>OpenDataSoft</td>\n",
       "      <td>['v1.0', 'v2.0', 'v2.1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>https://www.resourcedata.org</td>\n",
       "      <td>True</td>\n",
       "      <td>CKAN</td>\n",
       "      <td>2.7.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>https://zabal-agriculture.opendata-paysbasque.fr</td>\n",
       "      <td>True</td>\n",
       "      <td>OpenDataSoft</td>\n",
       "      <td>['v1.0', 'v2.0', 'v2.1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>https://zastrugis.my.opendatasoft.com</td>\n",
       "      <td>True</td>\n",
       "      <td>OpenDataSoft</td>\n",
       "      <td>['v1.0', 'v2.0', 'v2.1']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>705 rows Ã 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  api_working  \\\n",
       "0                                 http://101.79.9.128         True   \n",
       "1                                http://103.231.194.8         True   \n",
       "2                              http://116.203.208.239         True   \n",
       "3                               http://129.194.213.24         True   \n",
       "4                               http://130.179.67.140         True   \n",
       "5                                http://185.75.55.180         True   \n",
       "6                              http://192.241.154.193         True   \n",
       "7                                  http://3.237.95.96         True   \n",
       "8                                 http://84.38.48.220         True   \n",
       "9                            http://ambar.utpl.edu.ec         True   \n",
       "..                                                ...          ...   \n",
       "695                 https://www.offenesdatenportal.de         True   \n",
       "696                https://www.opendata-paysbasque.fr         True   \n",
       "697                      https://www.opendata.corsica         True   \n",
       "698                     https://www.opendata.nhs.scot         True   \n",
       "699                         https://www.opendata56.fr         True   \n",
       "700                       https://www.opentourism.net         True   \n",
       "701         https://www.projets-environnement.gouv.fr         True   \n",
       "702                      https://www.resourcedata.org         True   \n",
       "703  https://zabal-agriculture.opendata-paysbasque.fr         True   \n",
       "704             https://zastrugis.my.opendatasoft.com         True   \n",
       "\n",
       "     api_software               api_version  \n",
       "0            CKAN                     2.9.4  \n",
       "1            CKAN                     2.7.7  \n",
       "2            CKAN                     2.8.2  \n",
       "3            CKAN                     2.5.2  \n",
       "4            CKAN                     2.2.1  \n",
       "5            CKAN                     2.9.5  \n",
       "6            CKAN                     2.8.0  \n",
       "7            CKAN                     2.9.3  \n",
       "8            CKAN                       2.2  \n",
       "9            CKAN                     2.8.3  \n",
       "..            ...                       ...  \n",
       "695          CKAN                     2.8.2  \n",
       "696  OpenDataSoft  ['v1.0', 'v2.0', 'v2.1']  \n",
       "697  OpenDataSoft  ['v1.0', 'v2.0', 'v2.1']  \n",
       "698          CKAN                     2.8.4  \n",
       "699  OpenDataSoft  ['v1.0', 'v2.0', 'v2.1']  \n",
       "700          CKAN                     2.9.7  \n",
       "701  OpenDataSoft  ['v1.0', 'v2.0', 'v2.1']  \n",
       "702          CKAN                    2.7.11  \n",
       "703  OpenDataSoft  ['v1.0', 'v2.0', 'v2.1']  \n",
       "704  OpenDataSoft  ['v1.0', 'v2.0', 'v2.1']  \n",
       "\n",
       "[705 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calling the functions\n",
    "\"\"\"\n",
    "# Step 0\n",
    "extract_search_results(search_results_folder = \"../crawley-lite/results\", \n",
    "                    output_file = \"data/0_search_results.csv\")\n",
    "\n",
    "# Step 1\n",
    "# create_list(search_results_file = \"data/0_search_results.csv\", \n",
    "            output_file = \"data/1_initial_portals.csv\")\n",
    "\n",
    "# Step 2\n",
    "remove_duplicates(initial_portals_file = \"data/1_initial_portals.csv\",\n",
    "                output_file =  \"data/2_deduplicated_portals.csv\")\n",
    "\n",
    "# Step 3\n",
    "add_api_endpoints(manual_api_additions_file = \"data/manual_api_additions.csv\",\n",
    "                deduplicated_portals_file = \"data/2_deduplicated_portals.csv\",\n",
    "                output_file = \"data/3_extended_portals.csv\")\n",
    "\n",
    "# Step 4\n",
    "add_prefixes(extended_portals_file = \"data/3_extended_portals.csv\", \n",
    "            output_file = \"data/4_prefixed_portals.csv\")\n",
    "\n",
    "# Step 5\n",
    "validate_list(input_list = \"data/4_prefixed_portals.csv\",\n",
    "            output_list = \"data/5_validated_portals.csv\",\n",
    "            output_markers = \"data/5_validated_sites.json\")\n",
    "\n",
    "# Optional: Retry failed portals from step 5\n",
    "validate_list(input_list = \"data/5_validated_portals.csv\",\n",
    "            output_list = \"data/5_validated_portals_retry.csv\",\n",
    "            output_markers = \"data/5_validated_sites_retry.json\",\n",
    "            input_markers = \"data/5_validated_sites.json\",\n",
    "            retry_failed_portals = True)\n",
    "\n",
    "# Step 6\n",
    "analyze_list(validated_portals_file = \"data/5_validated_portals_retry.csv\", \n",
    "            show = True, \n",
    "            export = True)\n",
    "\n",
    "# Step 7\n",
    "extract_working_apis(validated_portals_file = \"data/5_validated_portals_retry.csv\", \n",
    "                    output_file = \"data/portals.csv\")\n",
    "\"\"\"\n",
    "\n",
    "with pd.option_context('display.min_rows', 20, 'display.max_columns', None):\n",
    "    display(pd.read_csv(\"data/portals.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
